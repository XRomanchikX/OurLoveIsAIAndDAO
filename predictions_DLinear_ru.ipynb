{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab3a30e5",
   "metadata": {},
   "source": [
    "# Бейзлайн решение\n",
    "\n",
    "В рамках олимпиады вам предстоит решить задачу по адаптация скорости LDPC-кодов для постобработки в квантовой криптографии.\n",
    "\n",
    "В данном Jupyter ноутбуке представлено бейзлайн решение, которое позволяет получить файл предсказания в нужном для проверяющей системы формате. Он тестировался с библиотеками из ```requirements.txt``` и версией ```Python 3.12.11```.\n",
    "\n",
    "#### Желаем удачи!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600387d6",
   "metadata": {},
   "source": [
    "# Препроцессинг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae68e000",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e11beae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество пропусков: 579\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"frames_errors.csv\", header=None)\n",
    "df.columns = [\n",
    "    \"block_id\",\n",
    "    \"frame_idx\",\n",
    "    \"E_mu_Z\",\n",
    "    \"E_mu_phys_est\",\n",
    "    \"E_mu_X\",\n",
    "    \"E_nu1_X\",\n",
    "    \"E_nu2_X\",\n",
    "    \"E_nu1_Z\",\n",
    "    \"E_nu2_Z\",\n",
    "    \"N_mu_X\",\n",
    "    \"M_mu_XX\",\n",
    "    \"M_mu_XZ\",\n",
    "    \"M_mu_X\",\n",
    "    \"N_mu_Z\",\n",
    "    \"M_mu_ZZ\",\n",
    "    \"M_mu_Z\",\n",
    "    \"N_nu1_X\",\n",
    "    \"M_nu1_XX\",\n",
    "    \"M_nu1_XZ\",\n",
    "    \"M_nu1_X\",\n",
    "    \"N_nu1_Z\",\n",
    "    \"M_nu1_ZZ\",\n",
    "    \"M_nu1_Z\",\n",
    "    \"N_nu2_X\",\n",
    "    \"M_nu2_XX\",\n",
    "    \"M_nu2_XZ\",\n",
    "    \"M_nu2_X\",\n",
    "    \"N_nu2_Z\",\n",
    "    \"M_nu2_ZZ\",\n",
    "    \"M_nu2_Z\",\n",
    "    \"nTot\",\n",
    "    \"bayesImVoltage\",\n",
    "    \"opticalPower\",\n",
    "    \"polarizerVoltages[0]\",\n",
    "    \"polarizerVoltages[1]\",\n",
    "    \"polarizerVoltages[2]\",\n",
    "    \"polarizerVoltages[3]\",\n",
    "    \"temp_1\",\n",
    "    \"biasVoltage_1\",\n",
    "    \"temp_2\",\n",
    "    \"biasVoltage_2\",\n",
    "    \"synErr\",\n",
    "    \"N_EC_rounds\",\n",
    "    \"maintenance_flag\",\n",
    "    \"estimator_name\",\n",
    "    \"f_EC\",\n",
    "    \"E_mu_Z_est\",\n",
    "    \"R\",\n",
    "    \"s\",\n",
    "    \"p\",\n",
    "]\n",
    "\n",
    "df_base = df.drop(\n",
    "    [\n",
    "        \"E_mu_phys_est\",\n",
    "        \"f_EC\",\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "print(f\"Количество пропусков: {df.isna().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cebaf76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_base.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f701bf11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество фреймов/Количество рядов\n",
      "date\n",
      "399    569\n",
      "400    251\n",
      "398      2\n",
      "390      1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = df.rename(\n",
    "    columns={\n",
    "        \"block_id\": \"id\",\n",
    "        \"E_mu_Z\": \"value\",\n",
    "        \"frame_idx\": \"date\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# Смотрим на длину временных рядов по количеству фреймов\n",
    "timestamp_counts = df.groupby(\"id\")[\"date\"].nunique()\n",
    "print(\"Количество фреймов/Количество рядов\")\n",
    "print(timestamp_counts.value_counts())\n",
    "\n",
    "df_for_ts = df[[\"id\", \"value\", \"date\"]].dropna(subset=[\"value\"], how=\"any\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e91ebc09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество фреймов/Количество рядов\n",
      "date\n",
      "400    815\n",
      "399      8\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_for_ts = df_for_ts.set_index([\"id\", \"date\"]).unstack().ffill().stack().reset_index()\n",
    "timestamp_counts = df_for_ts.groupby(\"id\")[\"date\"].nunique()\n",
    "print(\"Количество фреймов/Количество рядов\")\n",
    "print(timestamp_counts.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52d282ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Оставшиеся сегменты: 815\n"
     ]
    }
   ],
   "source": [
    "df_for_ts = df_for_ts.groupby(\"id\").filter(lambda x: len(x) == 400)\n",
    "print(\"Оставшиеся сегменты:\", df_for_ts[\"id\"].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030b1187",
   "metadata": {},
   "source": [
    "# DLinear\n",
    "\n",
    "`DLinear` — это простая и быстрая модель, которая выделяет тренд на основе AveragePooling, а затем на компонентах тренда и остатков применяет nn.Linear и собирает всё обратно. Ознакомиться с моделью можно в статье https://arxiv.org/abs/2205.13504"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49af1e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "import sys\n",
    "\n",
    "c_handler = logging.StreamHandler(sys.stdout)\n",
    "logger.addHandler(c_handler)\n",
    "logging.basicConfig(level=logging.INFO, force=True)\n",
    "\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.nn import Module\n",
    "\n",
    "from tsururu.dataset import Pipeline, TSDataset\n",
    "from tsururu.model_training.trainer import DLTrainer\n",
    "from tsururu.model_training.validator import HoldOutValidator\n",
    "from tsururu.strategies import RecursiveStrategy\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd932e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "class moving_avg(Module):\n",
    "    \"\"\"Блок скользящего среднего для выделения тренда временного ряда.\n",
    "\n",
    "    Аргументы:\n",
    "        kernel_size: размер окна свёртки (ядра).\n",
    "        stride: шаг скользящего среднего.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, kernel_size: int, stride: int):\n",
    "        super(moving_avg, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)\n",
    "\n",
    "    def forward(self, x: \"torch.Tensor\") -> \"torch.Tensor\":\n",
    "        \"\"\"Прямой проход для вычисления скользящего среднего.\n",
    "\n",
    "        Аргументы:\n",
    "            x: входной тензор.\n",
    "\n",
    "        Возвращает:\n",
    "            тензор после применения скользящего среднего.\n",
    "\n",
    "        \"\"\"\n",
    "        # добавляем паддинг (повторяем крайние значения) с обеих сторон временного ряда\n",
    "        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
    "        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
    "        x = torch.cat([front, x, end], dim=1)\n",
    "\n",
    "        # применяем скользящее среднее по временной оси\n",
    "        x = self.avg(x.permute(0, 2, 1))\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class series_decomp(Module):\n",
    "    \"\"\"Блок декомпозиции временного ряда.\n",
    "\n",
    "    Аргументы:\n",
    "        kernel_size: размер окна для скользящего среднего.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, kernel_size: int):\n",
    "        super(series_decomp, self).__init__()\n",
    "        self.moving_avg = moving_avg(kernel_size, stride=1)\n",
    "\n",
    "    def forward(self, x: \"torch.Tensor\") -> tuple[\"torch.Tensor\", \"torch.Tensor\"]:\n",
    "        \"\"\"Прямой проход для декомпозиции ряда на тренд и остаток.\n",
    "\n",
    "        Аргументы:\n",
    "            x: входной тензор.\n",
    "\n",
    "        Возвращает:\n",
    "            кортеж тензоров (остаток, тренд).\n",
    "\n",
    "        \"\"\"\n",
    "        moving_mean = self.moving_avg(x)\n",
    "        res = x - moving_mean\n",
    "\n",
    "        return res, moving_mean\n",
    "\n",
    "\n",
    "class DLinear_NN(Module):\n",
    "    def __init__(self, features_groups, pred_len, seq_len, moving_avg=25, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        # Защита от типовых «обёрток»\n",
    "        def _to_int(x):\n",
    "            if isinstance(x, int):\n",
    "                return x\n",
    "            if isinstance(x, dict) and \"value\" in x:\n",
    "                return int(x[\"value\"])\n",
    "            try:\n",
    "                return int(x)\n",
    "            except Exception:\n",
    "                raise TypeError(f\"Expected int-like, got {type(x)}: {x}\")\n",
    "\n",
    "        # Если вдруг пришли ещё и именованные — заберём их, чтобы не мешали\n",
    "        seq_len = _to_int(kwargs.pop(\"seq_len\", seq_len))\n",
    "        pred_len = _to_int(kwargs.pop(\"pred_len\", pred_len))\n",
    "        moving_avg = int(kwargs.pop(\"moving_avg\", moving_avg))\n",
    "\n",
    "        self.seq_len = seq_len\n",
    "        self.pred_len = pred_len\n",
    "\n",
    "        self.decompsition = series_decomp(moving_avg)\n",
    "        self.Linear_Seasonal = nn.Linear(self.seq_len, self.pred_len)\n",
    "        self.Linear_Trend = nn.Linear(self.seq_len, self.pred_len)\n",
    "\n",
    "        self.Linear_Seasonal.weight = nn.Parameter(\n",
    "            (1 / self.seq_len) * torch.ones([self.pred_len, self.seq_len])\n",
    "        )\n",
    "        self.Linear_Trend.weight = nn.Parameter(\n",
    "            (1 / self.seq_len) * torch.ones([self.pred_len, self.seq_len])\n",
    "        )\n",
    "\n",
    "    def forward(self, x: \"torch.Tensor\") -> \"torch.Tensor\":\n",
    "        \"\"\"Прямой проход модели.\n",
    "\n",
    "        Аргументы:\n",
    "            x: входной тензор формы (batch_size, seq_len, num_features).\n",
    "\n",
    "        Возвращает:\n",
    "            выходной тензор формы (batch_size, pred_len, num_features).\n",
    "\n",
    "        \"\"\"\n",
    "        # Декомпозиция временного ряда на тренд и остаток (сезонность)\n",
    "        seasonal_init, trend_init = self.decompsition(x)\n",
    "\n",
    "        # Транспонируем тензоры в формат (batch_size, num_features, seq_len)\n",
    "        seasonal_init, trend_init = seasonal_init.permute(0, 2, 1), trend_init.permute(\n",
    "            0, 2, 1\n",
    "        )\n",
    "\n",
    "        # Применяем линейные слои к тренду и остаткам\n",
    "        seasonal_output = self.Linear_Seasonal(seasonal_init)\n",
    "        trend_output = self.Linear_Trend(trend_init)\n",
    "\n",
    "        # Складываем результаты линейных слоёв\n",
    "        x = seasonal_output + trend_output\n",
    "\n",
    "        # Транспонируем обратно в формат (batch_size, seq_len, num_features)\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        return x[:, -self.pred_len :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7403fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Будем предсказывать 8 чисел вперед с окном 160\n",
    "\n",
    "HORIZON = 8\n",
    "HISTORY = 160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69f7bba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Форма обучающего набора: (312960, 3)\n",
      "Форма валидационного набора: (136920, 3)\n",
      "Форма тестового набора: (130400, 3)\n",
      "Форма целевых значений теста: (6520, 3)\n",
      "Количество рядов в обучающем наборе: 815\n",
      "Количество рядов в валидационном наборе: 815\n",
      "Количество рядов в тестовом наборе: 815\n",
      "Количество рядов в целевых значениях теста: 815\n"
     ]
    }
   ],
   "source": [
    "train_df = []\n",
    "val_df = []\n",
    "test_df = []\n",
    "test_targets = []\n",
    "for current_id in df_for_ts[\"id\"].unique():\n",
    "    current_df = df_for_ts[df_for_ts[\"id\"] == current_id]\n",
    "    train_df.append(current_df.iloc[: -2 * HORIZON])\n",
    "    val_df.append(current_df.iloc[-2 * HORIZON - HISTORY : -HORIZON])\n",
    "    test_df.append(current_df.iloc[-HORIZON - HISTORY : -HORIZON])\n",
    "    test_targets.append(current_df.iloc[-HORIZON:])\n",
    "train_df = pd.concat(train_df)\n",
    "val_df = pd.concat(val_df)\n",
    "test_df = pd.concat(test_df)\n",
    "test_targets = pd.concat(test_targets)\n",
    "\n",
    "\n",
    "print(f\"Форма обучающего набора: {train_df.shape}\")\n",
    "print(f\"Форма валидационного набора: {val_df.shape}\")\n",
    "print(f\"Форма тестового набора: {test_df.shape}\")\n",
    "print(f\"Форма целевых значений теста: {test_targets.shape}\")\n",
    "\n",
    "print(f\"Количество рядов в обучающем наборе: {train_df['id'].nunique()}\")\n",
    "print(f\"Количество рядов в валидационном наборе: {val_df['id'].nunique()}\")\n",
    "print(f\"Количество рядов в тестовом наборе: {test_df['id'].nunique()}\")\n",
    "print(f\"Количество рядов в целевых значениях теста: {test_targets['id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1aa379e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Устанавливаем базовую дату (первый день)\n",
    "# Это необходимо для корректной работы библиотеки tsururu и не влияет на суть задачи\n",
    "\n",
    "base_date = pd.to_datetime(\"2000-01-01\")\n",
    "\n",
    "\n",
    "def convert_dates(series):\n",
    "    return base_date + pd.to_timedelta(series.astype(int) - 1, unit=\"D\")\n",
    "\n",
    "\n",
    "# Применяем ко всем DataFrame\n",
    "\n",
    "train_df[\"date\"] = convert_dates(train_df[\"date\"])\n",
    "val_df[\"date\"] = convert_dates(val_df[\"date\"])\n",
    "test_df[\"date\"] = convert_dates(test_df[\"date\"])\n",
    "test_targets[\"date\"] = convert_dates(test_targets[\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65e36220",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18a8d352",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tsururu.dataset.dataset:freq: Day; period: 1\n"
     ]
    }
   ],
   "source": [
    "seed_everything()\n",
    "dataset_params = {\n",
    "    \"target\": {\n",
    "        \"columns\": [\"value\"],\n",
    "        \"type\": \"continuous\",\n",
    "    },\n",
    "    \"date\": {\n",
    "        \"columns\": [\"date\"],\n",
    "        \"type\": \"datetime\",\n",
    "    },\n",
    "    \"id\": {\n",
    "        \"columns\": [\"id\"],\n",
    "        \"type\": \"categorical\",\n",
    "    },\n",
    "}\n",
    "\n",
    "train_dataset = TSDataset(\n",
    "    data=train_df,\n",
    "    columns_params=dataset_params,\n",
    "    print_freq_period_info=True,\n",
    ")\n",
    "val_dataset = TSDataset(\n",
    "    data=val_df,\n",
    "    columns_params=dataset_params,\n",
    "    print_freq_period_info=False,\n",
    ")\n",
    "test_dataset = TSDataset(\n",
    "    data=test_df,\n",
    "    columns_params=dataset_params,\n",
    "    print_freq_period_info=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b90dfe21",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_params = {\n",
    "    \"target\": {\n",
    "        \"columns\": [\"value\"],\n",
    "        \"features\": {\n",
    "            \"DifferenceNormalizer\": {\n",
    "                \"regime\": \"delta\",\n",
    "                \"transform_target\": True,\n",
    "                \"transform_features\": True,\n",
    "            },\n",
    "            \"MissingValuesImputer\": {  # После DifferenceNormalizer у нас неизбежно появляются NaN в данных (в первом значении каждого сегмента)\n",
    "                \"constant_value\": 0,  # Заполним нулями\n",
    "                \"transform_target\": True,\n",
    "                \"transform_features\": True,\n",
    "            },\n",
    "            \"StandardScalerTransformer\": {  # И выровним значения рядов прежде чем подавать в DL модель\n",
    "                \"transform_target\": True,\n",
    "                \"transform_features\": True,\n",
    "                \"agg_by_id\": True,\n",
    "            },\n",
    "            \"LagTransformer\": {\"lags\": HISTORY},\n",
    "        },\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60966ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(\"Using GPU\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "        print(\"Using MPS\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"Using CPU\")\n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "701d5bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n"
     ]
    }
   ],
   "source": [
    "DEVICE = choose_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235ceb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Настроим обучение\n",
    "\n",
    "pipeline = Pipeline.from_dict(pipeline_params, multivariate=False)\n",
    "\n",
    "validation = HoldOutValidator\n",
    "validation_params = {\"validation_data\": val_dataset}\n",
    "\n",
    "trainer_params = {\n",
    "    \"device\": DEVICE,\n",
    "    \"num_workers\": 4,\n",
    "    \"best_by_metric\": True,\n",
    "    \"save_to_dir\": False,\n",
    "    \"batch_size\": 256,\n",
    "    \"n_epochs\": 5,\n",
    "    \"early_stopping_patience\": 2,\n",
    "}\n",
    "\n",
    "\n",
    "trainer = DLTrainer(\n",
    "    model=DLinear_NN,\n",
    "    model_params={\"moving_avg\": 25},\n",
    "    validator=validation,\n",
    "    validation_params=validation_params,\n",
    "    **trainer_params,\n",
    ")\n",
    "\n",
    "\n",
    "strategy = RecursiveStrategy(\n",
    "    horizon=HORIZON,\n",
    "    model_horizon=4,\n",
    "    history=HISTORY,\n",
    "    pipeline=pipeline,\n",
    "    trainer=trainer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4030151",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tsururu.model_training.trainer:length of train dataset: 180115\n",
      "INFO:tsururu.model_training.trainer:length of val dataset: 4075\n",
      "INFO:tsururu.model_training.trainer:Epoch 1/5, cost time: 96.20s\n",
      "INFO:tsururu.model_training.trainer:train loss: 0.7057\n",
      "INFO:tsururu.model_training.trainer:Validation, Loss: 0.6650, Metric: -0.6650\n",
      "INFO:tsururu.model_training.trainer:val loss: 0.6650, val metric: -0.6650\n",
      "INFO:tsururu.model_training.trainer:Epoch 2/5, cost time: 93.20s\n",
      "INFO:tsururu.model_training.trainer:train loss: 0.6827\n",
      "INFO:tsururu.model_training.trainer:Validation, Loss: 0.6613, Metric: -0.6613\n",
      "INFO:tsururu.model_training.trainer:val loss: 0.6613, val metric: -0.6613\n",
      "INFO:tsururu.model_training.trainer:Epoch 3/5, cost time: 90.28s\n",
      "INFO:tsururu.model_training.trainer:train loss: 0.6813\n",
      "INFO:tsururu.model_training.trainer:Validation, Loss: 0.6624, Metric: -0.6624\n",
      "INFO:tsururu.model_training.trainer:val loss: 0.6624, val metric: -0.6624\n",
      "INFO:tsururu.model_training.torch_based.callbacks:Early stopping counter: 1\n",
      "INFO:tsururu.model_training.trainer:Epoch 4/5, cost time: 90.33s\n",
      "INFO:tsururu.model_training.trainer:train loss: 0.6809\n",
      "INFO:tsururu.model_training.trainer:Validation, Loss: 0.6622, Metric: -0.6622\n",
      "INFO:tsururu.model_training.trainer:val loss: 0.6622, val metric: -0.6622\n",
      "INFO:tsururu.model_training.torch_based.callbacks:Early stopping counter: 2\n",
      "INFO:tsururu.model_training.torch_based.callbacks:Early stopping triggered\n",
      "INFO:tsururu.model_training.torch_based.callbacks:Training finished.\n",
      "INFO:tsururu.model_training.trainer:Fold 0. Score: -0.6621605753898621\n",
      "INFO:tsururu.model_training.trainer:Mean score: -0.6622\n",
      "INFO:tsururu.model_training.trainer:Std: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Запустим обучение\n",
    "\n",
    "fit_time, metrics = strategy.fit(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d6a003af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохраним модель для предоставления весов жюри\n",
    "\n",
    "import pickle\n",
    "\n",
    "model_filename = \"dlinear_strategy.pkl\"\n",
    "with open(model_filename, \"wb\") as f:\n",
    "    pickle.dump(strategy, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1a94f8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Для предсказания загрузим уже обученную модель\n",
    "\n",
    "with open(model_filename, \"rb\") as f:\n",
    "    loaded_strategy = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6b704b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tsururu.dataset.dataset:freq: Day; period: 1\n",
      "INFO:tsururu.model_training.trainer:length of test dataset: 815\n",
      "INFO:tsururu.model_training.trainer:length of test dataset: 815\n"
     ]
    }
   ],
   "source": [
    "forecast_time, current_pred = loaded_strategy.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ee464704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17612792</td>\n",
       "      <td>2001-01-26</td>\n",
       "      <td>0.018134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17612792</td>\n",
       "      <td>2001-01-27</td>\n",
       "      <td>0.017268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17612792</td>\n",
       "      <td>2001-01-28</td>\n",
       "      <td>0.018611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17612792</td>\n",
       "      <td>2001-01-29</td>\n",
       "      <td>0.020405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17612792</td>\n",
       "      <td>2001-01-30</td>\n",
       "      <td>0.019927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6515</th>\n",
       "      <td>2146878613</td>\n",
       "      <td>2001-01-29</td>\n",
       "      <td>0.023127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6516</th>\n",
       "      <td>2146878613</td>\n",
       "      <td>2001-01-30</td>\n",
       "      <td>0.02234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6517</th>\n",
       "      <td>2146878613</td>\n",
       "      <td>2001-01-31</td>\n",
       "      <td>0.02058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6518</th>\n",
       "      <td>2146878613</td>\n",
       "      <td>2001-02-01</td>\n",
       "      <td>0.018715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6519</th>\n",
       "      <td>2146878613</td>\n",
       "      <td>2001-02-02</td>\n",
       "      <td>0.017496</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6520 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id       date     value\n",
       "0       17612792 2001-01-26  0.018134\n",
       "1       17612792 2001-01-27  0.017268\n",
       "2       17612792 2001-01-28  0.018611\n",
       "3       17612792 2001-01-29  0.020405\n",
       "4       17612792 2001-01-30  0.019927\n",
       "...          ...        ...       ...\n",
       "6515  2146878613 2001-01-29  0.023127\n",
       "6516  2146878613 2001-01-30   0.02234\n",
       "6517  2146878613 2001-01-31   0.02058\n",
       "6518  2146878613 2001-02-01  0.018715\n",
       "6519  2146878613 2001-02-02  0.017496\n",
       "\n",
       "[6520 rows x 3 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "690158eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_pred = current_pred.sort_values([\"id\", \"date\"]).reset_index(drop=True)\n",
    "\n",
    "ids = current_pred[\"id\"].unique().tolist()\n",
    "n_ids = len(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "59e53869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Нам нужно вернуть 2000 точек значений\n",
    "TOTAL = 2000\n",
    "base = TOTAL // n_ids  # базовое число точек на id\n",
    "rem = TOTAL % n_ids  # первым rem id дадим на 1 точку больше\n",
    "\n",
    "if base == 0:\n",
    "    # Случай, если рядов слишком много (n_ids > 2000): берём по 1 точке для первых 2000 id\n",
    "    selected_ids = ids[:TOTAL]\n",
    "    compressed_values = []\n",
    "    for i in selected_ids:\n",
    "        arr = current_pred.loc[current_pred[\"id\"] == i, \"value\"].to_numpy()\n",
    "        # берём, например, последнее значение горизонта\n",
    "        compressed_values.append(float(arr[-1]))\n",
    "else:\n",
    "    # Обычный случай (~815 рядов): base=2, rem=2000-2*815=370 => 370 рядов дадут 3 точки, остальные 2\n",
    "    compressed_values = []\n",
    "    for idx, i in enumerate(ids):\n",
    "        k = base + (1 if idx < rem else 0)  # целевых точек для этого id\n",
    "        arr = current_pred.loc[current_pred[\"id\"] == i, \"value\"].to_numpy()\n",
    "\n",
    "        # Защита: если горизонт < k (не должно быть), просто повторим последние\n",
    "        if len(arr) < k:\n",
    "            arr = np.pad(arr, (0, k - len(arr)), mode=\"edge\")\n",
    "\n",
    "        # Режем на k ~равных кусков и усредняем каждый\n",
    "        chunks = np.array_split(arr, k)\n",
    "        means = [float(np.mean(c)) for c in chunks]\n",
    "        compressed_values.extend(means)\n",
    "\n",
    "# Получаем ровно 2000 значений в фиксированном порядке\n",
    "target_df = pd.DataFrame({\"value\": compressed_values})\n",
    "assert len(target_df) == 2000, f\"Got {len(target_df)} instead of 2000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "41cfd674",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "\n",
    "alpha = 0.33\n",
    "f_ec = 1.15\n",
    "R_range = [\n",
    "    round(0.50 + 0.05 * x, 2) for x in range(9)\n",
    "]  # 0.50..0.90 для соответствия условию задачи\n",
    "n = 32000\n",
    "d = 4800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4e88b42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ema(prev_ema, current_value, alpha):\n",
    "    if prev_ema is None:\n",
    "        return current_value\n",
    "    return alpha * current_value + (1 - alpha) * prev_ema\n",
    "\n",
    "\n",
    "def h(x):\n",
    "    if x > 0:\n",
    "        return -x * np.log2(x) - (1 - x) * np.log2(1 - x)\n",
    "    elif x == 0:\n",
    "        return 0.0\n",
    "    else:\n",
    "        raise ValueError(\"Invalid x for binary entropy\")\n",
    "\n",
    "\n",
    "def select_code_rate(e_mu, f_ec, rates, frame_len, sp_count):\n",
    "    r_candidate = 1 - h(e_mu) * f_ec\n",
    "    R_res = 0.50\n",
    "    s_n = sp_count\n",
    "    p_n = 0\n",
    "    for R in rates:\n",
    "        p_n = int(\n",
    "            ceil((1 - R) * frame_len - (1 - r_candidate) * (frame_len - sp_count))\n",
    "        )\n",
    "        s_n = int(sp_count - p_n)\n",
    "        if p_n >= 0 and s_n >= 0:\n",
    "            R_res = R\n",
    "            return round(R_res, 2), s_n, p_n\n",
    "    return round(R_res, 2), s_n, p_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "555058a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "E_series = (\n",
    "    pd.to_numeric(target_df.iloc[:, 0], errors=\"coerce\").dropna().reset_index(drop=True)\n",
    ")\n",
    "\n",
    "prev_ema = None\n",
    "rows = []\n",
    "for E_mu_Z in E_series:\n",
    "    ema_value = calculate_ema(prev_ema, float(E_mu_Z), alpha)\n",
    "    prev_ema = ema_value\n",
    "    R, s_n, p_n = select_code_rate(ema_value, f_ec, R_range, n, d)\n",
    "    rows.append([f\"{E_mu_Z:.16f}\", R, s_n, p_n])  # 4 столбца: E, R, s_n, p_n\n",
    "\n",
    "# Сохраним результат\n",
    "pd.DataFrame(rows).to_csv(\"submission.csv\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c56e55c",
   "metadata": {},
   "source": [
    "*Увеличение количества признаков*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f870253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество пропусков: 579\n",
      "Используется 16 признаков:\n",
      "Оставшиеся сегменты: 815\n",
      "Форма обучающего набора: (312960, 18)\n",
      "Используется признаков: 16\n",
      "freq: Day; period: 1\n",
      "Using GPU\n",
      "Обучение улучшенной NLinear модели с расширенными настройками...\n",
      "length of train dataset: 221\n",
      "length of val dataset: 5\n",
      "Epoch 1/15, cost time: 48.37s\n",
      "train loss: 1.9644\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "torch.cat(): expected a non-empty list of Tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 366\u001b[0m\n\u001b[0;32m    364\u001b[0m \u001b[38;5;66;03m# Обучение\u001b[39;00m\n\u001b[0;32m    365\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mОбучение улучшенной NLinear модели с расширенными настройками...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 366\u001b[0m fit_time, metrics \u001b[38;5;241m=\u001b[39m \u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    367\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mОбучение заняло \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfit_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m секунд\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    368\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mМетрики: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetrics\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Компуктер\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tsururu\\strategies\\utils.py:8\u001b[0m, in \u001b[0;36mtiming_decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m      7\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m----> 8\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     10\u001b[0m     execution_time \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[1;32mc:\\Users\\Компуктер\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tsururu\\strategies\\recursive.py:140\u001b[0m, in \u001b[0;36mRecursiveStrategy.fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pretrained_path:\n\u001b[0;32m    138\u001b[0m         current_trainer\u001b[38;5;241m.\u001b[39mpretrained_path \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrainer_0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 140\u001b[0m \u001b[43mcurrent_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipeline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(current_trainer, DLTrainer):\n\u001b[0;32m    143\u001b[0m     current_trainer\u001b[38;5;241m.\u001b[39mcheckpoint_path \u001b[38;5;241m=\u001b[39m checkpoint_path\n",
      "File \u001b[1;32mc:\\Users\\Компуктер\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tsururu\\model_training\\trainer.py:592\u001b[0m, in \u001b[0;36mDLTrainer.fit\u001b[1;34m(self, data, pipeline, val_data)\u001b[0m\n\u001b[0;32m    588\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpretrained_path:\n\u001b[0;32m    589\u001b[0m     model, optimizer, scheduler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_trainer_one_fold(\n\u001b[0;32m    590\u001b[0m         fold_i, model, optimizer, scheduler\n\u001b[0;32m    591\u001b[0m     )\n\u001b[1;32m--> 592\u001b[0m model, optimizer, scheduler, score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    593\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\n\u001b[0;32m    594\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    596\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mappend(model)\n\u001b[0;32m    597\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mappend(optimizer)\n",
      "File \u001b[1;32mc:\\Users\\Компуктер\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tsururu\\model_training\\trainer.py:433\u001b[0m, in \u001b[0;36mDLTrainer.train_model\u001b[1;34m(self, train_loader, val_loader, model, optimizer, scheduler)\u001b[0m\n\u001b[0;32m    430\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, cost time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    431\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 433\u001b[0m val_loss, val_metric \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    434\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, val metric: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_metric\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    436\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler_after_epoch \u001b[38;5;129;01mand\u001b[39;00m scheduler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Компуктер\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tsururu\\model_training\\trainer.py:511\u001b[0m, in \u001b[0;36mDLTrainer.validate_model\u001b[1;34m(self, val_loader, model, return_outputs, inference)\u001b[0m\n\u001b[0;32m    508\u001b[0m         all_outputs\u001b[38;5;241m.\u001b[39mappend(outputs)\n\u001b[0;32m    509\u001b[0m         all_targets\u001b[38;5;241m.\u001b[39mappend(targets)\n\u001b[1;32m--> 511\u001b[0m all_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    512\u001b[0m all_targets \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(all_targets)\n\u001b[0;32m    513\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(all_outputs, all_targets)\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[1;31mRuntimeError\u001b[0m: torch.cat(): expected a non-empty list of Tensors"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb652253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество пропусков: 579\n",
      "Количество фреймов/Количество рядов\n",
      "date\n",
      "399    569\n",
      "400    251\n",
      "398      2\n",
      "390      1\n",
      "Name: count, dtype: int64\n",
      "Количество фреймов/Количество рядов\n",
      "date\n",
      "400    815\n",
      "399      8\n",
      "Name: count, dtype: int64\n",
      "Оставшиеся сегменты: 815\n",
      "Форма обучающего набора: (312960, 3)\n",
      "Форма валидационного набора: (136920, 3)\n",
      "Форма тестового набора: (130400, 3)\n",
      "Форма целевых значений теста: (6520, 3)\n",
      "Количество рядов в обучающем наборе: 815\n",
      "Количество рядов в валидационном наборе: 815\n",
      "Количество рядов в тестовом наборе: 815\n",
      "Количество рядов в целевых значениях теста: 815\n",
      "freq: Day; period: 1\n",
      "Using GPU\n",
      "Обучение усиленной модели...\n",
      "length of train dataset: 180115\n",
      "length of val dataset: 4075\n",
      "Epoch 1/5, cost time: 99.14s\n",
      "train loss: 0.6834\n",
      "Validation, Loss: 0.6370, Metric: -0.6370\n",
      "val loss: 0.6370, val metric: -0.6370\n",
      "Epoch 2/5, cost time: 97.77s\n",
      "train loss: 0.5926\n",
      "Validation, Loss: 0.5288, Metric: -0.5288\n",
      "val loss: 0.5288, val metric: -0.5288\n",
      "Epoch 3/5, cost time: 98.60s\n",
      "train loss: 0.5401\n",
      "Validation, Loss: 0.4985, Metric: -0.4985\n",
      "val loss: 0.4985, val metric: -0.4985\n",
      "Epoch 4/5, cost time: 97.54s\n",
      "train loss: 0.5148\n",
      "Validation, Loss: 0.4853, Metric: -0.4853\n",
      "val loss: 0.4853, val metric: -0.4853\n",
      "Epoch 5/5, cost time: 97.63s\n",
      "train loss: 0.5006\n",
      "Validation, Loss: 0.4920, Metric: -0.4920\n",
      "val loss: 0.4920, val metric: -0.4920\n",
      "Early stopping counter: 1\n",
      "Training finished.\n",
      "Fold 0. Score: -0.4920431673526764\n",
      "Mean score: -0.492\n",
      "Std: 0.0\n",
      "Обучение заняло 591.25 секунд\n",
      "Метрики: <tsururu.strategies.recursive.RecursiveStrategy object at 0x00000221F0FBA1B0>\n",
      "freq: Day; period: 1\n",
      "length of test dataset: 815\n",
      "length of test dataset: 815\n",
      "Усиленная модель обучена и submission создан!\n",
      "Использована та же логика, что и в базовом решении, только модель заменена на EnhancedTemporalModel\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# БАЗОВОЕ РЕШЕНИЕ С УСИЛЕННОЙ МОДЕЛЬЮ (БЕЗ ДОПОЛНИТЕЛЬНЫХ ПРИЗНАКОВ)\n",
    "# =============================================================================\n",
    "\n",
    "import warnings\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Загрузка и подготовка данных (ТОЧНО КАК В БАЗОВОМ РЕШЕНИИ)\n",
    "df = pd.read_csv(\"frames_errors.csv\", header=None)\n",
    "df.columns = [\n",
    "    \"block_id\", \"frame_idx\", \"E_mu_Z\", \"E_mu_phys_est\", \"E_mu_X\", \"E_nu1_X\", \"E_nu2_X\", \n",
    "    \"E_nu1_Z\", \"E_nu2_Z\", \"N_mu_X\", \"M_mu_XX\", \"M_mu_XZ\", \"M_mu_X\", \"N_mu_Z\", \"M_mu_ZZ\", \n",
    "    \"M_mu_Z\", \"N_nu1_X\", \"M_nu1_XX\", \"M_nu1_XZ\", \"M_nu1_X\", \"N_nu1_Z\", \"M_nu1_ZZ\", \n",
    "    \"M_nu1_Z\", \"N_nu2_X\", \"M_nu2_XX\", \"M_nu2_XZ\", \"M_nu2_X\", \"N_nu2_Z\", \"M_nu2_ZZ\", \n",
    "    \"M_nu2_Z\", \"nTot\", \"bayesImVoltage\", \"opticalPower\", \"polarizerVoltages[0]\", \n",
    "    \"polarizerVoltages[1]\", \"polarizerVoltages[2]\", \"polarizerVoltages[3]\", \"temp_1\", \n",
    "    \"biasVoltage_1\", \"temp_2\", \"biasVoltage_2\", \"synErr\", \"N_EC_rounds\", \"maintenance_flag\", \n",
    "    \"estimator_name\", \"f_EC\", \"E_mu_Z_est\", \"R\", \"s\", \"p\"\n",
    "]\n",
    "\n",
    "df_base = df.drop([\"E_mu_phys_est\", \"f_EC\"], axis=1)\n",
    "print(f\"Количество пропусков: {df.isna().sum().sum()}\")\n",
    "\n",
    "df = df_base.copy()\n",
    "\n",
    "df = df.rename(columns={\"block_id\": \"id\", \"E_mu_Z\": \"value\", \"frame_idx\": \"date\"})\n",
    "\n",
    "# Смотрим на длину временных рядов по количеству фреймов\n",
    "timestamp_counts = df.groupby(\"id\")[\"date\"].nunique()\n",
    "print(\"Количество фреймов/Количество рядов\")\n",
    "print(timestamp_counts.value_counts())\n",
    "\n",
    "df_for_ts = df[[\"id\", \"value\", \"date\"]].dropna(subset=[\"value\"], how=\"any\")\n",
    "df_for_ts = df_for_ts.set_index([\"id\", \"date\"]).unstack().ffill().stack().reset_index()\n",
    "timestamp_counts = df_for_ts.groupby(\"id\")[\"date\"].nunique()\n",
    "print(\"Количество фреймов/Количество рядов\")\n",
    "print(timestamp_counts.value_counts())\n",
    "df_for_ts = df_for_ts.groupby(\"id\").filter(lambda x: len(x) == 400)\n",
    "print(\"Оставшиеся сегменты:\", df_for_ts[\"id\"].nunique())\n",
    "\n",
    "import logging\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Module\n",
    "from tsururu.dataset import Pipeline, TSDataset\n",
    "from tsururu.model_training.trainer import DLTrainer\n",
    "from tsururu.model_training.validator import HoldOutValidator\n",
    "from tsururu.strategies import RecursiveStrategy\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# =============================================================================\n",
    "# УСИЛЕННАЯ МОДЕЛЬ ВМЕСТО DLinear\n",
    "# =============================================================================\n",
    "\n",
    "class EnhancedTemporalModel(Module):\n",
    "    \"\"\"\n",
    "    Усиленная модель для временных рядов с механизмами внимания\n",
    "    Комбинация CNN + LSTM + Attention\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, features_groups, pred_len, seq_len, **kwargs):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Параметры\n",
    "        self.seq_len = seq_len\n",
    "        self.pred_len = pred_len\n",
    "        self.hidden_dim = 128\n",
    "        self.num_layers = 2\n",
    "        \n",
    "        # 1. CNN для извлечения локальных временных паттернов\n",
    "        self.conv1d = nn.Sequential(\n",
    "            nn.Conv1d(1, 64, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Conv1d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.AdaptiveAvgPool1d(seq_len // 2)  # уменьшаем размерность\n",
    "        )\n",
    "        \n",
    "        # 2. LSTM для долгосрочных зависимостей\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=128,\n",
    "            hidden_size=self.hidden_dim,\n",
    "            num_layers=self.num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=0.2,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "        # 3. Механизм внимания для важных временных точек\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(self.hidden_dim * 2, 64),  # bidirectional -> *2\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        \n",
    "        # 4. Полносвязные слои для прогнозирования\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.hidden_dim * 2, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, pred_len)\n",
    "        )\n",
    "        \n",
    "        # Инициализация весов\n",
    "        self._initialize_weights()\n",
    "        \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Инициализация весов для лучшей сходимости\"\"\"\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.constant_(module.bias, 0)\n",
    "            elif isinstance(module, nn.LSTM):\n",
    "                for name, param in module.named_parameters():\n",
    "                    if 'weight' in name:\n",
    "                        nn.init.orthogonal_(param)\n",
    "                    elif 'bias' in name:\n",
    "                        nn.init.constant_(param, 0)\n",
    "                        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x shape: (batch_size, seq_len, 1)\n",
    "        \"\"\"\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        # 1. Применяем CNN\n",
    "        x_cnn = x.permute(0, 2, 1)  # (batch_size, 1, seq_len)\n",
    "        x_cnn = self.conv1d(x_cnn)   # (batch_size, 128, seq_len//2)\n",
    "        x_cnn = x_cnn.permute(0, 2, 1)  # (batch_size, seq_len//2, 128)\n",
    "        \n",
    "        # 2. Применяем LSTM\n",
    "        lstm_out, (h_n, c_n) = self.lstm(x_cnn)  # (batch_size, seq_len//2, hidden_dim*2)\n",
    "        \n",
    "        # 3. Применяем механизм внимания\n",
    "        attention_weights = self.attention(lstm_out)  # (batch_size, seq_len//2, 1)\n",
    "        context_vector = torch.sum(attention_weights * lstm_out, dim=1)  # (batch_size, hidden_dim*2)\n",
    "        \n",
    "        # 4. Прогнозирование\n",
    "        output = self.fc(context_vector)  # (batch_size, pred_len)\n",
    "        \n",
    "        return output.unsqueeze(-1)  # (batch_size, pred_len, 1)\n",
    "\n",
    "# =============================================================================\n",
    "# ОСТАЛЬНОЙ КОД ТОЧНО КАК В БАЗОВОМ РЕШЕНИИ\n",
    "# =============================================================================\n",
    "\n",
    "# Будем предсказывать 8 чисел вперед с окном 160\n",
    "HORIZON = 8\n",
    "HISTORY = 160\n",
    "\n",
    "train_df = []\n",
    "val_df = []\n",
    "test_df = []\n",
    "test_targets = []\n",
    "for current_id in df_for_ts[\"id\"].unique():\n",
    "    current_df = df_for_ts[df_for_ts[\"id\"] == current_id]\n",
    "    train_df.append(current_df.iloc[: -2 * HORIZON])\n",
    "    val_df.append(current_df.iloc[-2 * HORIZON - HISTORY : -HORIZON])\n",
    "    test_df.append(current_df.iloc[-HORIZON - HISTORY : -HORIZON])\n",
    "    test_targets.append(current_df.iloc[-HORIZON:])\n",
    "train_df = pd.concat(train_df)\n",
    "val_df = pd.concat(val_df)\n",
    "test_df = pd.concat(test_df)\n",
    "test_targets = pd.concat(test_targets)\n",
    "\n",
    "print(f\"Форма обучающего набора: {train_df.shape}\")\n",
    "print(f\"Форма валидационного набора: {val_df.shape}\")\n",
    "print(f\"Форма тестового набора: {test_df.shape}\")\n",
    "print(f\"Форма целевых значений теста: {test_targets.shape}\")\n",
    "\n",
    "print(f\"Количество рядов в обучающем наборе: {train_df['id'].nunique()}\")\n",
    "print(f\"Количество рядов в валидационном наборе: {val_df['id'].nunique()}\")\n",
    "print(f\"Количество рядов в тестовом наборе: {test_df['id'].nunique()}\")\n",
    "print(f\"Количество рядов в целевых значениях теста: {test_targets['id'].nunique()}\")\n",
    "\n",
    "# Устанавливаем базовую дату (первый день)\n",
    "base_date = pd.to_datetime(\"2000-01-01\")\n",
    "\n",
    "def convert_dates(series):\n",
    "    return base_date + pd.to_timedelta(series.astype(int) - 1, unit=\"D\")\n",
    "\n",
    "# Применяем ко всем DataFrame\n",
    "train_df[\"date\"] = convert_dates(train_df[\"date\"])\n",
    "val_df[\"date\"] = convert_dates(val_df[\"date\"])\n",
    "test_df[\"date\"] = convert_dates(test_df[\"date\"])\n",
    "test_targets[\"date\"] = convert_dates(test_targets[\"date\"])\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything()\n",
    "\n",
    "dataset_params = {\n",
    "    \"target\": {\n",
    "        \"columns\": [\"value\"],\n",
    "        \"type\": \"continuous\",\n",
    "    },\n",
    "    \"date\": {\n",
    "        \"columns\": [\"date\"],\n",
    "        \"type\": \"datetime\",\n",
    "    },\n",
    "    \"id\": {\n",
    "        \"columns\": [\"id\"],\n",
    "        \"type\": \"categorical\",\n",
    "    },\n",
    "}\n",
    "\n",
    "train_dataset = TSDataset(\n",
    "    data=train_df,\n",
    "    columns_params=dataset_params,\n",
    "    print_freq_period_info=True,\n",
    ")\n",
    "val_dataset = TSDataset(\n",
    "    data=val_df,\n",
    "    columns_params=dataset_params,\n",
    "    print_freq_period_info=False,\n",
    ")\n",
    "test_dataset = TSDataset(\n",
    "    data=test_df,\n",
    "    columns_params=dataset_params,\n",
    "    print_freq_period_info=False,\n",
    ")\n",
    "\n",
    "pipeline_params = {\n",
    "    \"target\": {\n",
    "        \"columns\": [\"value\"],\n",
    "        \"features\": {\n",
    "            \"DifferenceNormalizer\": {\n",
    "                \"regime\": \"delta\",\n",
    "                \"transform_target\": True,\n",
    "                \"transform_features\": True,\n",
    "            },\n",
    "            \"MissingValuesImputer\": {\n",
    "                \"constant_value\": 0,\n",
    "                \"transform_target\": True,\n",
    "                \"transform_features\": True,\n",
    "            },\n",
    "            \"StandardScalerTransformer\": {\n",
    "                \"transform_target\": True,\n",
    "                \"transform_features\": True,\n",
    "                \"agg_by_id\": True,\n",
    "            },\n",
    "            \"LagTransformer\": {\"lags\": HISTORY},\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "def choose_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(\"Using GPU\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "        print(\"Using MPS\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"Using CPU\")\n",
    "    return device\n",
    "\n",
    "DEVICE = choose_device()\n",
    "\n",
    "# Настроим обучение С УСИЛЕННОЙ МОДЕЛЬЮ\n",
    "pipeline = Pipeline.from_dict(pipeline_params, multivariate=False)\n",
    "\n",
    "validation = HoldOutValidator\n",
    "validation_params = {\"validation_data\": val_dataset}\n",
    "\n",
    "# Параметры обучения (можно оставить как в базовом решении)\n",
    "trainer_params = {\n",
    "    \"device\": DEVICE,\n",
    "    \"num_workers\": 4,\n",
    "    \"best_by_metric\": True,\n",
    "    \"save_to_dir\": False,\n",
    "    \"batch_size\": 128,\n",
    "    \"n_epochs\": 5,\n",
    "    \"early_stopping_patience\": 2,\n",
    "}\n",
    "\n",
    "# ЕДИНСТВЕННОЕ ИЗМЕНЕНИЕ: используем усиленную модель вместо DLinear\n",
    "trainer = DLTrainer(\n",
    "    model=EnhancedTemporalModel,  # ← ЗДЕСЬ ЕДИНСТВЕННАЯ ЗАМЕНА\n",
    "    model_params={},\n",
    "    validator=validation,\n",
    "    validation_params=validation_params,\n",
    "    **trainer_params,\n",
    ")\n",
    "\n",
    "strategy = RecursiveStrategy(\n",
    "    horizon=HORIZON,\n",
    "    model_horizon=4,\n",
    "    history=HISTORY,\n",
    "    pipeline=pipeline,\n",
    "    trainer=trainer,\n",
    ")\n",
    "\n",
    "# Запустим обучение\n",
    "print(\"Обучение усиленной модели...\")\n",
    "fit_time, metrics = strategy.fit(train_dataset)\n",
    "print(f\"Обучение заняло {fit_time:.2f} секунд\")\n",
    "print(f\"Метрики: {metrics}\")\n",
    "\n",
    "# Сохраним модель\n",
    "import pickle\n",
    "model_filename = \"enhanced_temporal_model.pkl\"\n",
    "with open(model_filename, \"wb\") as f:\n",
    "    pickle.dump(strategy, f)\n",
    "\n",
    "# Для предсказания загрузим уже обученную модель\n",
    "with open(model_filename, \"rb\") as f:\n",
    "    loaded_strategy = pickle.load(f)\n",
    "\n",
    "forecast_time, current_pred = loaded_strategy.predict(test_dataset)\n",
    "\n",
    "current_pred = current_pred.sort_values([\"id\", \"date\"]).reset_index(drop=True)\n",
    "\n",
    "ids = current_pred[\"id\"].unique().tolist()\n",
    "n_ids = len(ids)\n",
    "\n",
    "# Нам нужно вернуть 2000 точек значений\n",
    "TOTAL = 2000\n",
    "base = TOTAL // n_ids\n",
    "rem = TOTAL % n_ids\n",
    "\n",
    "if base == 0:\n",
    "    selected_ids = ids[:TOTAL]\n",
    "    compressed_values = []\n",
    "    for i in selected_ids:\n",
    "        arr = current_pred.loc[current_pred[\"id\"] == i, \"value\"].to_numpy()\n",
    "        compressed_values.append(float(arr[-1]))\n",
    "else:\n",
    "    compressed_values = []\n",
    "    for idx, i in enumerate(ids):\n",
    "        k = base + (1 if idx < rem else 0)\n",
    "        arr = current_pred.loc[current_pred[\"id\"] == i, \"value\"].to_numpy()\n",
    "\n",
    "        if len(arr) < k:\n",
    "            arr = np.pad(arr, (0, k - len(arr)), mode=\"edge\")\n",
    "\n",
    "        chunks = np.array_split(arr, k)\n",
    "        means = [float(np.mean(c)) for c in chunks]\n",
    "        compressed_values.extend(means)\n",
    "\n",
    "# Получаем ровно 2000 значений\n",
    "target_df = pd.DataFrame({\"value\": compressed_values})\n",
    "assert len(target_df) == 2000, f\"Got {len(target_df)} instead of 2000\"\n",
    "\n",
    "from math import ceil\n",
    "\n",
    "alpha = 0.33\n",
    "f_ec = 1.15\n",
    "R_range = [round(0.50 + 0.05 * x, 2) for x in range(9)]\n",
    "n = 32000\n",
    "d = 4800\n",
    "\n",
    "def calculate_ema(prev_ema, current_value, alpha):\n",
    "    if prev_ema is None:\n",
    "        return current_value\n",
    "    return alpha * current_value + (1 - alpha) * prev_ema\n",
    "\n",
    "def h(x):\n",
    "    if x > 0:\n",
    "        return -x * np.log2(x) - (1 - x) * np.log2(1 - x)\n",
    "    elif x == 0:\n",
    "        return 0.0\n",
    "    else:\n",
    "        raise ValueError(\"Invalid x for binary entropy\")\n",
    "\n",
    "def select_code_rate(e_mu, f_ec, rates, frame_len, sp_count):\n",
    "    r_candidate = 1 - h(e_mu) * f_ec\n",
    "    R_res = 0.50\n",
    "    s_n = sp_count\n",
    "    p_n = 0\n",
    "    for R in rates:\n",
    "        p_n = int(ceil((1 - R) * frame_len - (1 - r_candidate) * (frame_len - sp_count)))\n",
    "        s_n = int(sp_count - p_n)\n",
    "        if p_n >= 0 and s_n >= 0:\n",
    "            R_res = R\n",
    "            return round(R_res, 2), s_n, p_n\n",
    "    return round(R_res, 2), s_n, p_n\n",
    "\n",
    "E_series = (\n",
    "    pd.to_numeric(target_df.iloc[:, 0], errors=\"coerce\").dropna().reset_index(drop=True)\n",
    ")\n",
    "\n",
    "prev_ema = None\n",
    "rows = []\n",
    "for E_mu_Z in E_series:\n",
    "    ema_value = calculate_ema(prev_ema, float(E_mu_Z), alpha)\n",
    "    prev_ema = ema_value\n",
    "    R, s_n, p_n = select_code_rate(ema_value, f_ec, R_range, n, d)\n",
    "    rows.append([f\"{E_mu_Z:.16f}\", R, s_n, p_n])\n",
    "\n",
    "# Сохраним результат\n",
    "pd.DataFrame(rows).to_csv(\"submission_enhanced_model.csv\", header=False, index=False)\n",
    "\n",
    "print(\"Усиленная модель обучена и submission создан!\")\n",
    "print(\"Использована та же логика, что и в базовом решении, только модель заменена на EnhancedTemporalModel\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
