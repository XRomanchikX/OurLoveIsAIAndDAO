{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bc0dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from core.tools.create_submission import create_submission\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4eb2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare_data():\n",
    "    df = pd.read_csv(\"frames_errors.csv\", header=None)\n",
    "    df.columns = [\n",
    "        \"block_id\", \"frame_idx\", \"E_mu_Z\", \"E_mu_phys_est\", \"E_mu_X\", \"E_nu1_X\", \"E_nu2_X\", \n",
    "        \"E_nu1_Z\", \"E_nu2_Z\", \"N_mu_X\", \"M_mu_XX\", \"M_mu_XZ\", \"M_mu_X\", \"N_mu_Z\", \"M_mu_ZZ\", \n",
    "        \"M_mu_Z\", \"N_nu1_X\", \"M_nu1_XX\", \"M_nu1_XZ\", \"M_nu1_X\", \"N_nu1_Z\", \"M_nu1_ZZ\", \n",
    "        \"M_nu1_Z\", \"N_nu2_X\", \"M_nu2_XX\", \"M_nu2_XZ\", \"M_nu2_X\", \"N_nu2_Z\", \"M_nu2_ZZ\", \n",
    "        \"M_nu2_Z\", \"nTot\", \"bayesImVoltage\", \"opticalPower\", \"polarizerVoltages[0]\", \n",
    "        \"polarizerVoltages[1]\", \"polarizerVoltages[2]\", \"polarizerVoltages[3]\", \"temp_1\", \n",
    "        \"biasVoltage_1\", \"temp_2\", \"biasVoltage_2\", \"synErr\", \"N_EC_rounds\", \"maintenance_flag\", \n",
    "        \"estimator_name\", \"f_EC\", \"E_mu_Z_est\", \"R\", \"s\", \"p\"\n",
    "    ]\n",
    "    \n",
    "    df_base = df.drop([\"E_mu_phys_est\", \"f_EC\"], axis=1)\n",
    "    print(f\"Количество пропусков: {df.isna().sum().sum()}\")\n",
    "    \n",
    "    return df_base\n",
    "\n",
    "def create_features(df):\n",
    "    \"\"\"Создание расширенных признаков\"\"\"\n",
    "    df_featured = df.copy()\n",
    "    \n",
    "    # Основные QBER признаки\n",
    "    df_featured['E_mu_avg'] = (df_featured['E_mu_Z'] + df_featured['E_mu_X']) / 2\n",
    "    df_featured['E_mu_diff'] = df_featured['E_mu_Z'] - df_featured['E_mu_X']\n",
    "    \n",
    "    # Статистики по состояниям\n",
    "    df_featured['total_sent'] = df_featured[['N_mu_X', 'N_mu_Z']].sum(axis=1)\n",
    "    df_featured['mu_ratio'] = (df_featured['N_mu_X'] + df_featured['N_mu_Z']) / (df_featured['total_sent'] + 1e-8)\n",
    "    \n",
    "    # Физические параметры\n",
    "    df_featured['temp_avg'] = (df_featured['temp_1'] + df_featured['temp_2']) / 2\n",
    "    df_featured['bias_avg'] = (df_featured['biasVoltage_1'] + df_featured['biasVoltage_2']) / 2\n",
    "    \n",
    "    return df_featured\n",
    "\n",
    "def prepare_time_series_data(df, target_column='E_mu_Z', sequence_length=160, horizon=8):\n",
    "    \"\"\"Подготовка данных для временных рядов\"\"\"\n",
    "    \n",
    "    # Переименование\n",
    "    df = df.rename(columns={\"block_id\": \"id\", \"frame_idx\": \"date\", target_column: \"value\"})\n",
    "    \n",
    "    # Выбор признаков\n",
    "    feature_columns = [\n",
    "        'value', 'E_mu_X', 'E_mu_avg', 'E_mu_diff', \n",
    "        'total_sent', 'mu_ratio', 'temp_avg', 'bias_avg', 'opticalPower'\n",
    "    ]\n",
    "    existing_features = [col for col in feature_columns if col in df.columns]\n",
    "    \n",
    "    print(f\"Используется {len(existing_features)} признаков: {existing_features}\")\n",
    "    \n",
    "    # Создание датафрейма с выбранными признаками\n",
    "    df_for_ts = df[['id', 'date'] + existing_features].dropna(subset=['value'], how='any')\n",
    "    \n",
    "    # Обработка временных рядов\n",
    "    df_for_ts = df_for_ts.set_index(['id', 'date']).unstack().ffill().stack().reset_index()\n",
    "    df_for_ts = df_for_ts.groupby('id').filter(lambda x: len(x) == 400)\n",
    "    \n",
    "    print(f\"Оставшиеся сегменты: {df_for_ts['id'].nunique()}\")\n",
    "    \n",
    "    return df_for_ts, existing_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029f7f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    \"\"\"Кастомный Dataset для временных рядов\"\"\"\n",
    "    \n",
    "    def __init__(self, data, sequence_length=160, horizon=8, target_col='value', scale=True):\n",
    "        self.data = data\n",
    "        self.sequence_length = sequence_length\n",
    "        self.horizon = horizon\n",
    "        self.target_col = target_col\n",
    "        self.scale = scale\n",
    "        \n",
    "        self.groups = []\n",
    "        self.scalers = {}\n",
    "        \n",
    "        for device_id in data['id'].unique():\n",
    "            device_data = data[data['id'] == device_id].sort_values('date')\n",
    "            \n",
    "            feature_cols = [col for col in device_data.columns if col not in ['id', 'date', target_col]]\n",
    "            features = device_data[feature_cols].values\n",
    "            target = device_data[target_col].values\n",
    "            \n",
    "            if scale:\n",
    "                feature_scaler = StandardScaler()\n",
    "                target_scaler = StandardScaler()\n",
    "                \n",
    "                features_scaled = feature_scaler.fit_transform(features)\n",
    "                target_scaled = target_scaler.fit_transform(target.reshape(-1, 1)).flatten()\n",
    "                \n",
    "                self.scalers[device_id] = (feature_scaler, target_scaler)\n",
    "            else:\n",
    "                features_scaled = features\n",
    "                target_scaled = target\n",
    "                self.scalers[device_id] = (None, None)\n",
    "            \n",
    "            sequences = []\n",
    "            targets = []\n",
    "            \n",
    "            for i in range(len(device_data) - sequence_length - horizon + 1):\n",
    "                seq_features = features_scaled[i:i + sequence_length]\n",
    "                seq_target = target_scaled[i + sequence_length:i + sequence_length + horizon]\n",
    "                \n",
    "                sequences.append(seq_features)\n",
    "                targets.append(seq_target)\n",
    "            \n",
    "            if sequences:\n",
    "                self.groups.append({\n",
    "                    'device_id': device_id,\n",
    "                    'sequences': np.array(sequences, dtype=np.float32),\n",
    "                    'targets': np.array(targets, dtype=np.float32)\n",
    "                })\n",
    "\n",
    "        self.all_sequences = np.concatenate([group['sequences'] for group in self.groups])\n",
    "        self.all_targets = np.concatenate([group['targets'] for group in self.groups])\n",
    "        \n",
    "        print(f\"Всего последовательностей: {len(self.all_sequences)}\")\n",
    "        print(f\"Форма sequences: {self.all_sequences.shape}\")\n",
    "        print(f\"Форма targets: {self.all_targets.shape}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.all_sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.all_sequences[idx] \n",
    "        target = self.all_targets[idx]      \n",
    "        \n",
    "        sequence_tensor = torch.FloatTensor(sequence)\n",
    "        target_tensor = torch.FloatTensor(target)\n",
    "        \n",
    "        return sequence_tensor, target_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80e971b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.tools.metrics import *\n",
    "\n",
    "def train_model(model, train_loader, val_loader, device, epochs=10, lr=1e-3):\n",
    "    \"\"\"Обучение модели с метриками BER, SNR, SE и Decoding Complexity\"\"\"\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.96, nesterov=True)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, factor=0.5)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    # Метрики\n",
    "    train_ber_values = []\n",
    "    val_ber_values = []\n",
    "    train_snr_values = []\n",
    "    val_snr_values = []\n",
    "    train_se_values = []\n",
    "    val_se_values = []\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience = 5\n",
    "    patience_counter = 0\n",
    "    \n",
    "    sample_batch = next(iter(train_loader))\n",
    "    sequence_length = sample_batch[0].shape[1]\n",
    "    decoding_complexity = calculate_decoding_complexity(model, sequence_length)\n",
    "    \n",
    "    print(f\"Decoding Complexity Metrics:\")\n",
    "    print(f\"  Total Parameters: {decoding_complexity['total_parameters']:,}\")\n",
    "    print(f\"  Computational Complexity: {decoding_complexity['computational_complexity']:,}\")\n",
    "    print(f\"  Complexity Score: {decoding_complexity['complexity_score']:,}\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Обучение\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_ber = 0.0\n",
    "        train_snr = 0.0\n",
    "        train_se = 0.0\n",
    "        \n",
    "        for sequences, targets in train_loader:\n",
    "            sequences = sequences.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(sequences)\n",
    "            predictions = outputs[:, :, 0]\n",
    "            \n",
    "            loss = criterion(predictions, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                ber = calculate_ber(predictions, targets)\n",
    "                snr = calculate_snr(predictions, targets)\n",
    "                se = calculate_spectral_efficiency(predictions, targets)\n",
    "                \n",
    "                train_ber += ber\n",
    "                train_snr += snr\n",
    "                train_se += se\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_ber = 0.0\n",
    "        val_snr = 0.0\n",
    "        val_se = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for sequences, targets in val_loader:\n",
    "                sequences = sequences.to(device)\n",
    "                targets = targets.to(device)\n",
    "                \n",
    "                outputs = model(sequences)\n",
    "                predictions = outputs[:, :, 0]\n",
    "                \n",
    "                loss = criterion(predictions, targets)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                ber = calculate_ber(predictions, targets)\n",
    "                snr = calculate_snr(predictions, targets)\n",
    "                se = calculate_spectral_efficiency(predictions, targets)\n",
    "                \n",
    "                val_ber += ber\n",
    "                val_snr += snr\n",
    "                val_se += se\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        train_ber /= len(train_loader)\n",
    "        val_ber /= len(val_loader)\n",
    "        train_snr /= len(train_loader)\n",
    "        val_snr /= len(val_loader)\n",
    "        train_se /= len(train_loader)\n",
    "        val_se /= len(val_loader)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_ber_values.append(train_ber)\n",
    "        val_ber_values.append(val_ber)\n",
    "        train_snr_values.append(train_snr)\n",
    "        val_snr_values.append(val_snr)\n",
    "        train_se_values.append(train_se)\n",
    "        val_se_values.append(val_se)\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{epochs}:')\n",
    "        print(f'  Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}')\n",
    "        print(f'  Train BER: {train_ber:.6f}, Val BER: {val_ber:.6f}')\n",
    "        print(f'  Train SNR: {train_snr:.4f} dB, Val SNR: {val_snr:.4f} dB')\n",
    "        print(f'  Train SE: {train_se:.6f}, Val SE: {val_se:.6f}')\n",
    "        print(f'  LR: {optimizer.param_groups[0][\"lr\"]:.2e}')\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(f'Early stopping at epoch {epoch+1}')\n",
    "            break\n",
    "    \n",
    "    model.load_state_dict(torch.load('best_model.pth'))\n",
    "    \n",
    "    metrics = {\n",
    "        'train_ber': train_ber_values,\n",
    "        'val_ber': val_ber_values,\n",
    "        'train_snr': train_snr_values,\n",
    "        'val_snr': val_snr_values,\n",
    "        'train_se': train_se_values,\n",
    "        'val_se': val_se_values,\n",
    "        'decoding_complexity': decoding_complexity\n",
    "    }\n",
    "    \n",
    "    return model, train_losses, val_losses, metrics\n",
    "\n",
    "def predict(model, test_loader, device):\n",
    "    \"\"\"Прогнозирование на тестовых данных\"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for sequences, targets in test_loader:\n",
    "            sequences = sequences.to(device)\n",
    "            \n",
    "            outputs = model(sequences)\n",
    "            preds = outputs[:, :, 0].cpu().numpy()\n",
    "            \n",
    "            predictions.extend(preds)\n",
    "    \n",
    "    return np.array(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e26bd2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество пропусков: 579\n",
      "Используется 9 признаков: ['value', 'E_mu_X', 'E_mu_avg', 'E_mu_diff', 'total_sent', 'mu_ratio', 'temp_avg', 'bias_avg', 'opticalPower']\n",
      "Оставшиеся сегменты: 815\n",
      "Всего последовательностей: 132810\n",
      "Форма sequences: (132810, 160, 8)\n",
      "Форма targets: (132810, 8)\n",
      "Всего последовательностей: 28426\n",
      "Форма sequences: (28426, 160, 8)\n",
      "Форма targets: (28426, 8)\n",
      "Всего последовательностей: 28659\n",
      "Форма sequences: (28659, 160, 8)\n",
      "Форма targets: (28659, 8)\n",
      "Количество параметров: 22256\n",
      "Decoding Complexity Metrics:\n",
      "  Total Parameters: 22,256\n",
      "  Computational Complexity: 3,560,960\n",
      "  Complexity Score: 3,583,216\n",
      "Epoch 1/55:\n",
      "  Train Loss: 1.010401, Val Loss: 0.934059\n",
      "  Train BER: 0.283140, Val BER: 0.267432\n",
      "  Train SNR: -0.0619 dB, Val SNR: 0.2408 dB\n",
      "  Train SE: 0.075672, Val SE: 0.072733\n",
      "  LR: 1.00e-03\n",
      "Epoch 2/55:\n",
      "  Train Loss: 0.947037, Val Loss: 0.903128\n",
      "  Train BER: 0.270168, Val BER: 0.267193\n",
      "  Train SNR: 0.2115 dB, Val SNR: 0.3852 dB\n",
      "  Train SE: 0.079642, Val SE: 0.076933\n",
      "  LR: 1.00e-03\n"
     ]
    }
   ],
   "source": [
    "from model import DLinear_NN\n",
    "\n",
    "SEQUENCE_LENGTH = 160\n",
    "HORIZON = 8\n",
    "BATCH_SIZE = 1280\n",
    "EPOCHS = 55\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "df = load_and_prepare_data()\n",
    "df = create_features(df)\n",
    "df_for_ts, feature_columns = prepare_time_series_data(df)\n",
    "\n",
    "device_ids = df_for_ts['id'].unique()\n",
    "train_devices = device_ids[:int(0.7 * len(device_ids))]\n",
    "val_devices = device_ids[int(0.7 * len(device_ids)):int(0.85 * len(device_ids))]\n",
    "test_devices = device_ids[int(0.85 * len(device_ids)):]\n",
    "\n",
    "train_data = df_for_ts[df_for_ts['id'].isin(train_devices)]\n",
    "val_data = df_for_ts[df_for_ts['id'].isin(val_devices)]\n",
    "test_data = df_for_ts[df_for_ts['id'].isin(test_devices)]\n",
    "\n",
    "train_dataset = TimeSeriesDataset(train_data, SEQUENCE_LENGTH, HORIZON)\n",
    "val_dataset = TimeSeriesDataset(val_data, SEQUENCE_LENGTH, HORIZON)\n",
    "test_dataset = TimeSeriesDataset(test_data, SEQUENCE_LENGTH, HORIZON)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Модель\n",
    "num_features = len(feature_columns) - 1\n",
    "model = DLinear_NN(\n",
    "    pred_len=8,        \n",
    "    seq_len=160,       \n",
    "    num_features=num_features,\n",
    ").to(device)\n",
    "\n",
    "print(f\"Количество параметров: {sum(p.numel() for p in model.parameters())}\")\n",
    "\n",
    "model, train_losses, val_losses = train_model(\n",
    "    model, train_loader, val_loader, device, EPOCHS, LEARNING_RATE\n",
    ")\n",
    "\n",
    "print(\"3. Прогнозирование...\")\n",
    "test_predictions = predict(model, test_loader, device)\n",
    "\n",
    "test_predictions_original = []\n",
    "for i, device_id in enumerate(test_devices):\n",
    "    if device_id in test_dataset.scalers:\n",
    "        _, target_scaler = test_dataset.scalers[device_id]\n",
    "        if target_scaler:\n",
    "            device_preds = test_predictions[i * (400 - SEQUENCE_LENGTH - HORIZON + 1): \n",
    "                                            (i + 1) * (400 - SEQUENCE_LENGTH - HORIZON + 1)]\n",
    "            last_preds = device_preds[:, -1].reshape(-1, 1)\n",
    "            original_scale = target_scaler.inverse_transform(last_preds).flatten()\n",
    "            test_predictions_original.extend(original_scale)\n",
    "\n",
    "print(\"4. Создание submission...\")\n",
    "create_submission(test_predictions_original)\n",
    "\n",
    "print(\"Готово!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
