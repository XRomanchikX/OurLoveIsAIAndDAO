{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31bc0dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from core.tools.create_submission import create_submission\n",
    "from math import ceil\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe4eb2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare_data():\n",
    "    df = pd.read_csv(\"frames_errors.csv\", header=None)\n",
    "    df.columns = [\n",
    "        \"block_id\", \"frame_idx\", \"E_mu_Z\", \"E_mu_phys_est\", \"E_mu_X\", \"E_nu1_X\", \"E_nu2_X\", \n",
    "        \"E_nu1_Z\", \"E_nu2_Z\", \"N_mu_X\", \"M_mu_XX\", \"M_mu_XZ\", \"M_mu_X\", \"N_mu_Z\", \"M_mu_ZZ\", \n",
    "        \"M_mu_Z\", \"N_nu1_X\", \"M_nu1_XX\", \"M_nu1_XZ\", \"M_nu1_X\", \"N_nu1_Z\", \"M_nu1_ZZ\", \n",
    "        \"M_nu1_Z\", \"N_nu2_X\", \"M_nu2_XX\", \"M_nu2_XZ\", \"M_nu2_X\", \"N_nu2_Z\", \"M_nu2_ZZ\", \n",
    "        \"M_nu2_Z\", \"nTot\", \"bayesImVoltage\", \"opticalPower\", \"polarizerVoltages[0]\", \n",
    "        \"polarizerVoltages[1]\", \"polarizerVoltages[2]\", \"polarizerVoltages[3]\", \"temp_1\", \n",
    "        \"biasVoltage_1\", \"temp_2\", \"biasVoltage_2\", \"synErr\", \"N_EC_rounds\", \"maintenance_flag\", \n",
    "        \"estimator_name\", \"f_EC\", \"E_mu_Z_est\", \"R\", \"s\", \"p\"\n",
    "    ]\n",
    "    \n",
    "    df_base = df.drop([\"E_mu_phys_est\", \"f_EC\"], axis=1)\n",
    "    \n",
    "    return df_base\n",
    "\n",
    "def create_features(df):\n",
    "    \"\"\"Создание расширенных признаков\"\"\"\n",
    "    df_featured = df.copy()\n",
    "    \n",
    "    # Основные QBER признаки\n",
    "    df_featured['E_mu_avg'] = (df_featured['E_mu_Z'] + df_featured['E_mu_X']) / 2\n",
    "    df_featured['E_mu_diff'] = df_featured['E_mu_Z'] - df_featured['E_mu_X']\n",
    "    \n",
    "    # Статистики по состояниям\n",
    "    df_featured['total_sent'] = df_featured[['N_mu_X', 'N_mu_Z']].sum(axis=1)\n",
    "    df_featured['mu_ratio'] = (df_featured['N_mu_X'] + df_featured['N_mu_Z']) / (df_featured['total_sent'] + 1e-8)\n",
    "    \n",
    "    # Физические параметры\n",
    "    df_featured['temp_avg'] = (df_featured['temp_1'] + df_featured['temp_2']) / 2\n",
    "    df_featured['bias_avg'] = (df_featured['biasVoltage_1'] + df_featured['biasVoltage_2']) / 2\n",
    "    \n",
    "    return df_featured\n",
    "\n",
    "def prepare_time_series_data(df, target_column='E_mu_Z', sequence_length=160, horizon=8):\n",
    "    \"\"\"Подготовка данных для временных рядов\"\"\"\n",
    "    \n",
    "    # Переименование\n",
    "    df = df.rename(columns={\"block_id\": \"id\", \"frame_idx\": \"date\", target_column: \"value\"})\n",
    "    \n",
    "    # Выбор признаков\n",
    "    feature_columns = [\n",
    "        'value', 'E_mu_X', 'E_mu_avg', 'E_mu_diff', \n",
    "        'total_sent', 'mu_ratio', 'temp_avg', 'bias_avg', 'opticalPower'\n",
    "    ]\n",
    "    existing_features = [col for col in feature_columns if col in df.columns]\n",
    "    \n",
    "    # Создание датафрейма с выбранными признаками\n",
    "    df_for_ts = df[['id', 'date'] + existing_features].dropna(subset=['value'], how='any')\n",
    "    \n",
    "    # Обработка временных рядов\n",
    "    df_for_ts = df_for_ts.set_index(['id', 'date']).unstack().ffill().stack().reset_index()\n",
    "    df_for_ts = df_for_ts.groupby('id').filter(lambda x: len(x) == 400)\n",
    "    \n",
    "    return df_for_ts, existing_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "029f7f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    \"\"\"Кастомный Dataset для временных рядов\"\"\"\n",
    "    \n",
    "    def __init__(self, data, sequence_length=160, horizon=8, target_col='value', scale=True):\n",
    "        self.data = data\n",
    "        self.sequence_length = sequence_length\n",
    "        self.horizon = horizon\n",
    "        self.target_col = target_col\n",
    "        self.scale = scale\n",
    "        \n",
    "        self.groups = []\n",
    "        self.scalers = {}\n",
    "        \n",
    "        for device_id in data['id'].unique():\n",
    "            device_data = data[data['id'] == device_id].sort_values('date')\n",
    "            \n",
    "            feature_cols = [col for col in device_data.columns if col not in ['id', 'date', target_col]]\n",
    "            features = device_data[feature_cols].values\n",
    "            target = device_data[target_col].values\n",
    "            \n",
    "            if scale:\n",
    "                feature_scaler = StandardScaler()\n",
    "                target_scaler = StandardScaler()\n",
    "                \n",
    "                features_scaled = feature_scaler.fit_transform(features)\n",
    "                target_scaled = target_scaler.fit_transform(target.reshape(-1, 1)).flatten()\n",
    "                \n",
    "                self.scalers[device_id] = (feature_scaler, target_scaler)\n",
    "            else:\n",
    "                features_scaled = features\n",
    "                target_scaled = target\n",
    "                self.scalers[device_id] = (None, None)\n",
    "            \n",
    "            sequences = []\n",
    "            targets = []\n",
    "            \n",
    "            for i in range(len(device_data) - sequence_length - horizon + 1):\n",
    "                seq_features = features_scaled[i:i + sequence_length]\n",
    "                seq_target = target_scaled[i + sequence_length:i + sequence_length + horizon]\n",
    "                \n",
    "                sequences.append(seq_features)\n",
    "                targets.append(seq_target)\n",
    "            \n",
    "            if sequences:\n",
    "                self.groups.append({\n",
    "                    'device_id': device_id,\n",
    "                    'sequences': np.array(sequences, dtype=np.float32),\n",
    "                    'targets': np.array(targets, dtype=np.float32)\n",
    "                })\n",
    "\n",
    "        self.all_sequences = np.concatenate([group['sequences'] for group in self.groups])\n",
    "        self.all_targets = np.concatenate([group['targets'] for group in self.groups])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.all_sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.all_sequences[idx] \n",
    "        target = self.all_targets[idx]      \n",
    "        \n",
    "        sequence_tensor = torch.FloatTensor(sequence)\n",
    "        target_tensor = torch.FloatTensor(target)\n",
    "        \n",
    "        return sequence_tensor, target_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f80e971b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.tools.metrics import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "def calculate_secret_key_length(corrected_key_length: int, \n",
    "                               reconciliation_info: int, \n",
    "                               privacy_amplification_loss: int) -> int:\n",
    "    \"\"\"\n",
    "    Вычисляет длину секретного ключа.\n",
    "    \"\"\"\n",
    "    secret_key_length = max(0, corrected_key_length - reconciliation_info - privacy_amplification_loss)\n",
    "    return secret_key_length\n",
    "\n",
    "def compute_qkd_metrics(corrected_keys: list, reconciliation_infos: list, privacy_losses: list) -> dict:\n",
    "    \"\"\"\n",
    "    Вычисляет QKD метрики: Public и Private.\n",
    "    corrected_keys: [len1, len2, len3, len4, len5] — длины после коррекции\n",
    "    reconciliation_infos: [info1, info2, ...]\n",
    "    privacy_losses: [loss1, loss2, ...]\n",
    "    \"\"\"\n",
    "    # Public metric: на 2 фиксированных сегментах\n",
    "    public_metric = 0\n",
    "    for i in range(2):\n",
    "        sk_len = calculate_secret_key_length(\n",
    "            corrected_key_length=corrected_keys[i],\n",
    "            reconciliation_info=reconciliation_infos[i],\n",
    "            privacy_amplification_loss=privacy_losses[i]\n",
    "        )\n",
    "        public_metric += sk_len\n",
    "\n",
    "    # Private metric: на всех 5 сегментах\n",
    "    private_metric = 0\n",
    "    for i in range(5):\n",
    "        sk_len = calculate_secret_key_length(\n",
    "            corrected_key_length=corrected_keys[i],\n",
    "            reconciliation_info=reconciliation_infos[i],\n",
    "            privacy_amplification_loss=privacy_losses[i]\n",
    "        )\n",
    "        private_metric += sk_len\n",
    "\n",
    "    return {\n",
    "        \"public_metric\": public_metric,\n",
    "        \"private_metric\": private_metric\n",
    "    }\n",
    "\n",
    "def train_model(model: nn.Module, train_loader, val_loader, device, epochs=10, lr=1e-3):\n",
    "    \"\"\"Обучение модели с QKD метриками: public и private secret key length.\"\"\"\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, factor=0.5)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    # Метрики QKD\n",
    "    train_public_metrics = []\n",
    "    val_public_metrics = []\n",
    "    train_private_metrics = []\n",
    "    val_private_metrics = []\n",
    "    \n",
    "    best_private_metric = -float('inf')\n",
    "    patience = 25\n",
    "    patience_counter = 0\n",
    "    \n",
    "    sample_batch = next(iter(train_loader))\n",
    "    sequence_length = sample_batch[0].shape[1]\n",
    "    decoding_complexity = calculate_decoding_complexity(model, sequence_length)\n",
    "    \n",
    "    print(f\"Decoding Complexity Metrics:\")\n",
    "    print(f\"  Total Parameters: {decoding_complexity['total_parameters']:,}\")\n",
    "    print(f\"  Computational Complexity: {decoding_complexity['computational_complexity']:,}\")\n",
    "    print(f\"  Complexity Score: {decoding_complexity['complexity_score']:,}\")\n",
    "    \n",
    "    model.load_state_dict(torch.load('best_model.pth'))\n",
    "    for epoch in range(epochs):\n",
    "        # Обучение\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_public_metric = 0.0\n",
    "        train_private_metric = 0.0\n",
    "        \n",
    "        for sequences, targets in train_loader:\n",
    "            sequences = sequences.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(sequences)\n",
    "            predictions = outputs[:, :, 0]\n",
    "            \n",
    "            loss = criterion(predictions, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            # Дискретизация: предполагаем, что > 0.5 = 1, иначе = 0\n",
    "            pred_bits = (predictions > 0.5).float()\n",
    "            target_bits = (targets > 0.5).float()\n",
    "            \n",
    "            # Вычисляем QKD метрики для батча (псевдо-сегменты)\n",
    "            with torch.no_grad():\n",
    "                corrected_key_length = int(torch.sum((pred_bits == target_bits).float()).item())\n",
    "                \n",
    "                # Пример: на основе длины ключа вычисляем метрики\n",
    "                corrected_keys = [corrected_key_length] * 5  # 5 сегментов\n",
    "                reconciliation_infos = [50] * 5  # пример\n",
    "                privacy_losses = [20] * 5       # пример\n",
    "                qkd_metrics = compute_qkd_metrics(corrected_keys, reconciliation_infos, privacy_losses)\n",
    "                \n",
    "                train_public_metric += qkd_metrics[\"public_metric\"]\n",
    "                train_private_metric += qkd_metrics[\"private_metric\"]\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_public_metric = 0.0\n",
    "        val_private_metric = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for sequences, targets in val_loader:\n",
    "                sequences = sequences.to(device)\n",
    "                targets = targets.to(device)\n",
    "                \n",
    "                outputs = model(sequences)\n",
    "                predictions = outputs[:, :, 0]\n",
    "                \n",
    "                loss = criterion(predictions, targets)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                # Дискретизация\n",
    "                pred_bits = (predictions > 0.5).float()\n",
    "                target_bits = (targets > 0.5).float()\n",
    "                \n",
    "                corrected_key_length = int(torch.sum((pred_bits == target_bits).float()).item())\n",
    "                \n",
    "                corrected_keys = [corrected_key_length] * 5\n",
    "                reconciliation_infos = [50] * 5\n",
    "                privacy_losses = [20] * 5\n",
    "                qkd_metrics = compute_qkd_metrics(corrected_keys, reconciliation_infos, privacy_losses)\n",
    "                \n",
    "                val_public_metric += qkd_metrics[\"public_metric\"]\n",
    "                val_private_metric += qkd_metrics[\"private_metric\"]\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        train_public_metric /= len(train_loader)\n",
    "        val_public_metric /= len(val_loader)\n",
    "        train_private_metric /= len(train_loader)\n",
    "        val_private_metric /= len(val_loader)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_public_metrics.append(train_public_metric)\n",
    "        val_public_metrics.append(val_public_metric)\n",
    "        train_private_metrics.append(train_private_metric)\n",
    "        val_private_metrics.append(val_private_metric)\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Early stopping по private metric\n",
    "        if ceil(val_public_metric * 100) + ceil(val_private_metric * 100) > best_private_metric:\n",
    "            best_private_metric = ceil(val_public_metric * 100) + ceil(val_private_metric * 100)\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{epochs}:')\n",
    "        print(f'  Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}')\n",
    "        print(f'  Train Public Metric: {ceil(train_public_metric * 100) + ceil(train_private_metric * 100):,}, Val Public Metric: {ceil(val_public_metric * 100) + ceil(val_private_metric * 100):,}')\n",
    "        print(f'  LR: {optimizer.param_groups[0][\"lr\"]:.2e} | Patience Counter: {patience_counter}')\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(f'Early stopping at epoch {epoch+1}')\n",
    "            break\n",
    "    \n",
    "    metrics = {\n",
    "        'train_public_metric': train_public_metrics,\n",
    "        'val_public_metric': val_public_metrics,\n",
    "        'train_private_metric': train_private_metrics,\n",
    "        'val_private_metric': val_private_metrics,\n",
    "        'decoding_complexity': decoding_complexity\n",
    "    }\n",
    "    \n",
    "    return model, train_losses, val_losses\n",
    "\n",
    "def predict(model, test_loader, device):\n",
    "    \"\"\"Прогнозирование на тестовых данных\"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for sequences, targets in test_loader:\n",
    "            sequences = sequences.to(device)\n",
    "            \n",
    "            outputs = model(sequences)\n",
    "            preds = outputs[:, :, 0].cpu().numpy()\n",
    "            \n",
    "            predictions.extend(preds)\n",
    "    \n",
    "    return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e26bd2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество параметров: 172528\n",
      "3. Прогнозирование...\n",
      "4. Создание submission...\n",
      "Размер submission: 2000 строк\n",
      "Готово!\n"
     ]
    }
   ],
   "source": [
    "from model import DLinear_NN, DLinear_Qwen\n",
    "\n",
    "SEQUENCE_LENGTH = 160\n",
    "HORIZON = 8\n",
    "BATCH_SIZE = 1280\n",
    "EPOCHS = 500\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "df = load_and_prepare_data()\n",
    "df = create_features(df)\n",
    "df_for_ts, feature_columns = prepare_time_series_data(df)\n",
    "\n",
    "device_ids = df_for_ts['id'].unique()\n",
    "train_devices = device_ids[:int(0.7 * len(device_ids))]\n",
    "val_devices = device_ids[int(0.7 * len(device_ids)):int(0.85 * len(device_ids))]\n",
    "test_devices = device_ids[int(0.85 * len(device_ids)):]\n",
    "\n",
    "train_data = df_for_ts[df_for_ts['id'].isin(train_devices)]\n",
    "val_data = df_for_ts[df_for_ts['id'].isin(val_devices)]\n",
    "test_data = df_for_ts[df_for_ts['id'].isin(test_devices)]\n",
    "\n",
    "train_dataset = TimeSeriesDataset(train_data, SEQUENCE_LENGTH, HORIZON)\n",
    "val_dataset = TimeSeriesDataset(val_data, SEQUENCE_LENGTH, HORIZON)\n",
    "test_dataset = TimeSeriesDataset(test_data, SEQUENCE_LENGTH, HORIZON)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Модель\n",
    "num_features = len(feature_columns) - 1\n",
    "model = DLinear_NN(\n",
    "    pred_len=8,        \n",
    "    seq_len=160,       \n",
    "    num_features=num_features,\n",
    ").to(device)\n",
    "\n",
    "#model = DLinear_Qwen(\n",
    "#    seq_len=160, \n",
    "#    pred_len=8, \n",
    "#    n_layers=4, \n",
    "#    d_model=256, \n",
    "#    activation=\"gelu\"\n",
    "#).to(device)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "print(f\"Количество параметров: {sum(p.numel() for p in model.parameters())}\")\n",
    "\n",
    "#model, train_losses, val_losses = train_model(\n",
    "#    model, train_loader, val_loader, device, EPOCHS, LEARNING_RATE\n",
    "#)\n",
    "\n",
    "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "\n",
    "print(\"3. Прогнозирование...\")\n",
    "test_predictions = predict(model, test_loader, device)\n",
    "\n",
    "test_predictions_original = []\n",
    "for i, device_id in enumerate(test_devices):\n",
    "    if device_id in test_dataset.scalers:\n",
    "        _, target_scaler = test_dataset.scalers[device_id]\n",
    "        if target_scaler:\n",
    "            device_preds = test_predictions[i * (400 - SEQUENCE_LENGTH - HORIZON + 1): \n",
    "                                            (i + 1) * (400 - SEQUENCE_LENGTH - HORIZON + 1)]\n",
    "            last_preds = device_preds[:, -1].reshape(-1, 1)\n",
    "            original_scale = target_scaler.inverse_transform(last_preds).flatten()\n",
    "            test_predictions_original.extend(original_scale)\n",
    "\n",
    "print(\"4. Создание submission...\")\n",
    "create_submission(test_predictions_original)\n",
    "\n",
    "print(\"Готово!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
