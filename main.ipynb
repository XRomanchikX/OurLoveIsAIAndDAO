{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31bc0dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from core.tools.create_submission import create_submission\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe4eb2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare_data():\n",
    "    df = pd.read_csv(\"frames_errors.csv\", header=None)\n",
    "    df.columns = [\n",
    "        \"block_id\", \"frame_idx\", \"E_mu_Z\", \"E_mu_phys_est\", \"E_mu_X\", \"E_nu1_X\", \"E_nu2_X\", \n",
    "        \"E_nu1_Z\", \"E_nu2_Z\", \"N_mu_X\", \"M_mu_XX\", \"M_mu_XZ\", \"M_mu_X\", \"N_mu_Z\", \"M_mu_ZZ\", \n",
    "        \"M_mu_Z\", \"N_nu1_X\", \"M_nu1_XX\", \"M_nu1_XZ\", \"M_nu1_X\", \"N_nu1_Z\", \"M_nu1_ZZ\", \n",
    "        \"M_nu1_Z\", \"N_nu2_X\", \"M_nu2_XX\", \"M_nu2_XZ\", \"M_nu2_X\", \"N_nu2_Z\", \"M_nu2_ZZ\", \n",
    "        \"M_nu2_Z\", \"nTot\", \"bayesImVoltage\", \"opticalPower\", \"polarizerVoltages[0]\", \n",
    "        \"polarizerVoltages[1]\", \"polarizerVoltages[2]\", \"polarizerVoltages[3]\", \"temp_1\", \n",
    "        \"biasVoltage_1\", \"temp_2\", \"biasVoltage_2\", \"synErr\", \"N_EC_rounds\", \"maintenance_flag\", \n",
    "        \"estimator_name\", \"f_EC\", \"E_mu_Z_est\", \"R\", \"s\", \"p\"\n",
    "    ]\n",
    "    \n",
    "    df_base = df.drop([\"E_mu_phys_est\", \"f_EC\"], axis=1)\n",
    "    print(f\"Количество пропусков: {df.isna().sum().sum()}\")\n",
    "    \n",
    "    return df_base\n",
    "\n",
    "def create_features(df):\n",
    "    \"\"\"Создание расширенных признаков\"\"\"\n",
    "    df_featured = df.copy()\n",
    "    \n",
    "    # Основные QBER признаки\n",
    "    df_featured['E_mu_avg'] = (df_featured['E_mu_Z'] + df_featured['E_mu_X']) / 2\n",
    "    df_featured['E_mu_diff'] = df_featured['E_mu_Z'] - df_featured['E_mu_X']\n",
    "    \n",
    "    # Статистики по состояниям\n",
    "    df_featured['total_sent'] = df_featured[['N_mu_X', 'N_mu_Z']].sum(axis=1)\n",
    "    df_featured['mu_ratio'] = (df_featured['N_mu_X'] + df_featured['N_mu_Z']) / (df_featured['total_sent'] + 1e-8)\n",
    "    \n",
    "    # Физические параметры\n",
    "    df_featured['temp_avg'] = (df_featured['temp_1'] + df_featured['temp_2']) / 2\n",
    "    df_featured['bias_avg'] = (df_featured['biasVoltage_1'] + df_featured['biasVoltage_2']) / 2\n",
    "    \n",
    "    return df_featured\n",
    "\n",
    "def prepare_time_series_data(df, target_column='E_mu_Z', sequence_length=160, horizon=8):\n",
    "    \"\"\"Подготовка данных для временных рядов\"\"\"\n",
    "    \n",
    "    # Переименование\n",
    "    df = df.rename(columns={\"block_id\": \"id\", \"frame_idx\": \"date\", target_column: \"value\"})\n",
    "    \n",
    "    # Выбор признаков\n",
    "    feature_columns = [\n",
    "        'value', 'E_mu_X', 'E_mu_avg', 'E_mu_diff', \n",
    "        'total_sent', 'mu_ratio', 'temp_avg', 'bias_avg', 'opticalPower'\n",
    "    ]\n",
    "    existing_features = [col for col in feature_columns if col in df.columns]\n",
    "    \n",
    "    print(f\"Используется {len(existing_features)} признаков: {existing_features}\")\n",
    "    \n",
    "    # Создание датафрейма с выбранными признаками\n",
    "    df_for_ts = df[['id', 'date'] + existing_features].dropna(subset=['value'], how='any')\n",
    "    \n",
    "    # Обработка временных рядов\n",
    "    df_for_ts = df_for_ts.set_index(['id', 'date']).unstack().ffill().stack().reset_index()\n",
    "    df_for_ts = df_for_ts.groupby('id').filter(lambda x: len(x) == 400)\n",
    "    \n",
    "    print(f\"Оставшиеся сегменты: {df_for_ts['id'].nunique()}\")\n",
    "    \n",
    "    return df_for_ts, existing_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "029f7f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    \"\"\"Кастомный Dataset для временных рядов\"\"\"\n",
    "    \n",
    "    def __init__(self, data, sequence_length=160, horizon=8, target_col='value', scale=True):\n",
    "        self.data = data\n",
    "        self.sequence_length = sequence_length\n",
    "        self.horizon = horizon\n",
    "        self.target_col = target_col\n",
    "        self.scale = scale\n",
    "        \n",
    "        self.groups = []\n",
    "        self.scalers = {}\n",
    "        \n",
    "        for device_id in data['id'].unique():\n",
    "            device_data = data[data['id'] == device_id].sort_values('date')\n",
    "            \n",
    "            feature_cols = [col for col in device_data.columns if col not in ['id', 'date', target_col]]\n",
    "            features = device_data[feature_cols].values\n",
    "            target = device_data[target_col].values\n",
    "            \n",
    "            if scale:\n",
    "                feature_scaler = StandardScaler()\n",
    "                target_scaler = StandardScaler()\n",
    "                \n",
    "                features_scaled = feature_scaler.fit_transform(features)\n",
    "                target_scaled = target_scaler.fit_transform(target.reshape(-1, 1)).flatten()\n",
    "                \n",
    "                self.scalers[device_id] = (feature_scaler, target_scaler)\n",
    "            else:\n",
    "                features_scaled = features\n",
    "                target_scaled = target\n",
    "                self.scalers[device_id] = (None, None)\n",
    "            \n",
    "            sequences = []\n",
    "            targets = []\n",
    "            \n",
    "            for i in range(len(device_data) - sequence_length - horizon + 1):\n",
    "                seq_features = features_scaled[i:i + sequence_length]\n",
    "                seq_target = target_scaled[i + sequence_length:i + sequence_length + horizon]\n",
    "                \n",
    "                sequences.append(seq_features)\n",
    "                targets.append(seq_target)\n",
    "            \n",
    "            if sequences:\n",
    "                self.groups.append({\n",
    "                    'device_id': device_id,\n",
    "                    'sequences': np.array(sequences, dtype=np.float32),\n",
    "                    'targets': np.array(targets, dtype=np.float32)\n",
    "                })\n",
    "\n",
    "        self.all_sequences = np.concatenate([group['sequences'] for group in self.groups])\n",
    "        self.all_targets = np.concatenate([group['targets'] for group in self.groups])\n",
    "        \n",
    "        print(f\"Всего последовательностей: {len(self.all_sequences)}\")\n",
    "        print(f\"Форма sequences: {self.all_sequences.shape}\")\n",
    "        print(f\"Форма targets: {self.all_targets.shape}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.all_sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.all_sequences[idx] \n",
    "        target = self.all_targets[idx]      \n",
    "        \n",
    "        sequence_tensor = torch.FloatTensor(sequence)\n",
    "        target_tensor = torch.FloatTensor(target)\n",
    "        \n",
    "        return sequence_tensor, target_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80e971b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.tools.metrics import *\n",
    "\n",
    "def train_model(model, train_loader, val_loader, device, epochs=10, lr=1e-3):\n",
    "    \"\"\"Обучение модели с метриками BER, SNR, SE и Decoding Complexity\"\"\"\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.96, nesterov=True)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, factor=0.5)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    # Метрики\n",
    "    train_ber_values = []\n",
    "    val_ber_values = []\n",
    "    train_snr_values = []\n",
    "    val_snr_values = []\n",
    "    train_se_values = []\n",
    "    val_se_values = []\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience = 5\n",
    "    patience_counter = 0\n",
    "    \n",
    "    sample_batch = next(iter(train_loader))\n",
    "    sequence_length = sample_batch[0].shape[1]\n",
    "    decoding_complexity = calculate_decoding_complexity(model, sequence_length)\n",
    "    \n",
    "    print(f\"Decoding Complexity Metrics:\")\n",
    "    print(f\"  Total Parameters: {decoding_complexity['total_parameters']:,}\")\n",
    "    print(f\"  Computational Complexity: {decoding_complexity['computational_complexity']:,}\")\n",
    "    print(f\"  Complexity Score: {decoding_complexity['complexity_score']:,}\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Обучение\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_ber = 0.0\n",
    "        train_snr = 0.0\n",
    "        train_se = 0.0\n",
    "        \n",
    "        for sequences, targets in train_loader:\n",
    "            sequences = sequences.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(sequences)\n",
    "            predictions = outputs[:, :, 0]\n",
    "            \n",
    "            loss = criterion(predictions, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                ber = calculate_ber(predictions, targets)\n",
    "                snr = calculate_snr(predictions, targets)\n",
    "                se = calculate_spectral_efficiency(predictions, targets)\n",
    "                \n",
    "                train_ber += ber\n",
    "                train_snr += snr\n",
    "                train_se += se\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_ber = 0.0\n",
    "        val_snr = 0.0\n",
    "        val_se = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for sequences, targets in val_loader:\n",
    "                sequences = sequences.to(device)\n",
    "                targets = targets.to(device)\n",
    "                \n",
    "                outputs = model(sequences)\n",
    "                predictions = outputs[:, :, 0]\n",
    "                \n",
    "                loss = criterion(predictions, targets)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                ber = calculate_ber(predictions, targets)\n",
    "                snr = calculate_snr(predictions, targets)\n",
    "                se = calculate_spectral_efficiency(predictions, targets)\n",
    "                \n",
    "                val_ber += ber\n",
    "                val_snr += snr\n",
    "                val_se += se\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        train_ber /= len(train_loader)\n",
    "        val_ber /= len(val_loader)\n",
    "        train_snr /= len(train_loader)\n",
    "        val_snr /= len(val_loader)\n",
    "        train_se /= len(train_loader)\n",
    "        val_se /= len(val_loader)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_ber_values.append(train_ber)\n",
    "        val_ber_values.append(val_ber)\n",
    "        train_snr_values.append(train_snr)\n",
    "        val_snr_values.append(val_snr)\n",
    "        train_se_values.append(train_se)\n",
    "        val_se_values.append(val_se)\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{epochs}:')\n",
    "        print(f'  Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}')\n",
    "        print(f'  Train BER: {train_ber:.6f}, Val BER: {val_ber:.6f}')\n",
    "        print(f'  Train SNR: {train_snr:.4f} dB, Val SNR: {val_snr:.4f} dB')\n",
    "        print(f'  Train SE: {train_se:.6f}, Val SE: {val_se:.6f}')\n",
    "        print(f'  LR: {optimizer.param_groups[0][\"lr\"]:.2e}')\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(f'Early stopping at epoch {epoch+1}')\n",
    "            break\n",
    "    \n",
    "    model.load_state_dict(torch.load('best_model.pth'))\n",
    "    \n",
    "    metrics = {\n",
    "        'train_ber': train_ber_values,\n",
    "        'val_ber': val_ber_values,\n",
    "        'train_snr': train_snr_values,\n",
    "        'val_snr': val_snr_values,\n",
    "        'train_se': train_se_values,\n",
    "        'val_se': val_se_values,\n",
    "        'decoding_complexity': decoding_complexity\n",
    "    }\n",
    "    \n",
    "    return model, train_losses, val_losses\n",
    "\n",
    "def predict(model, test_loader, device):\n",
    "    \"\"\"Прогнозирование на тестовых данных\"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for sequences, targets in test_loader:\n",
    "            sequences = sequences.to(device)\n",
    "            \n",
    "            outputs = model(sequences)\n",
    "            preds = outputs[:, :, 0].cpu().numpy()\n",
    "            \n",
    "            predictions.extend(preds)\n",
    "    \n",
    "    return np.array(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e26bd2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество пропусков: 579\n",
      "Используется 9 признаков: ['value', 'E_mu_X', 'E_mu_avg', 'E_mu_diff', 'total_sent', 'mu_ratio', 'temp_avg', 'bias_avg', 'opticalPower']\n",
      "Оставшиеся сегменты: 815\n",
      "Всего последовательностей: 132810\n",
      "Форма sequences: (132810, 160, 8)\n",
      "Форма targets: (132810, 8)\n",
      "Всего последовательностей: 28426\n",
      "Форма sequences: (28426, 160, 8)\n",
      "Форма targets: (28426, 8)\n",
      "Всего последовательностей: 28659\n",
      "Форма sequences: (28659, 160, 8)\n",
      "Форма targets: (28659, 8)\n",
      "Количество параметров: 22256\n",
      "Decoding Complexity Metrics:\n",
      "  Total Parameters: 22,256\n",
      "  Computational Complexity: 3,560,960\n",
      "  Complexity Score: 3,583,216\n",
      "Epoch 1/55:\n",
      "  Train Loss: 0.995537, Val Loss: 0.903030\n",
      "  Train BER: 0.273973, Val BER: 0.266751\n",
      "  Train SNR: 0.0116 dB, Val SNR: 0.3899 dB\n",
      "  Train SE: 0.081288, Val SE: 0.081102\n",
      "  LR: 1.00e-03\n",
      "Epoch 2/55:\n",
      "  Train Loss: 0.916952, Val Loss: 0.871749\n",
      "  Train BER: 0.265671, Val BER: 0.265053\n",
      "  Train SNR: 0.3517 dB, Val SNR: 0.5377 dB\n",
      "  Train SE: 0.084599, Val SE: 0.084523\n",
      "  LR: 1.00e-03\n",
      "Epoch 3/55:\n",
      "  Train Loss: 0.891206, Val Loss: 0.852297\n",
      "  Train BER: 0.262050, Val BER: 0.260478\n",
      "  Train SNR: 0.4754 dB, Val SNR: 0.6315 dB\n",
      "  Train SE: 0.087216, Val SE: 0.085972\n",
      "  LR: 1.00e-03\n",
      "Epoch 4/55:\n",
      "  Train Loss: 0.872794, Val Loss: 0.839130\n",
      "  Train BER: 0.258953, Val BER: 0.256150\n",
      "  Train SNR: 0.5668 dB, Val SNR: 0.6966 dB\n",
      "  Train SE: 0.089060, Val SE: 0.086267\n",
      "  LR: 1.00e-03\n",
      "Epoch 5/55:\n",
      "  Train Loss: 0.860815, Val Loss: 0.830085\n",
      "  Train BER: 0.256367, Val BER: 0.254803\n",
      "  Train SNR: 0.6272 dB, Val SNR: 0.7420 dB\n",
      "  Train SE: 0.089840, Val SE: 0.088014\n",
      "  LR: 1.00e-03\n",
      "Epoch 6/55:\n",
      "  Train Loss: 0.852118, Val Loss: 0.826481\n",
      "  Train BER: 0.254808, Val BER: 0.254344\n",
      "  Train SNR: 0.6712 dB, Val SNR: 0.7588 dB\n",
      "  Train SE: 0.091085, Val SE: 0.087370\n",
      "  LR: 1.00e-03\n",
      "Epoch 7/55:\n",
      "  Train Loss: 0.846021, Val Loss: 0.821716\n",
      "  Train BER: 0.253679, Val BER: 0.252663\n",
      "  Train SNR: 0.7020 dB, Val SNR: 0.7832 dB\n",
      "  Train SE: 0.091658, Val SE: 0.088152\n",
      "  LR: 1.00e-03\n",
      "Epoch 8/55:\n",
      "  Train Loss: 0.844810, Val Loss: 0.819023\n",
      "  Train BER: 0.253088, Val BER: 0.250961\n",
      "  Train SNR: 0.7090 dB, Val SNR: 0.7980 dB\n",
      "  Train SE: 0.091472, Val SE: 0.089304\n",
      "  LR: 1.00e-03\n",
      "Epoch 9/55:\n",
      "  Train Loss: 0.840284, Val Loss: 0.817734\n",
      "  Train BER: 0.251725, Val BER: 0.251706\n",
      "  Train SNR: 0.7325 dB, Val SNR: 0.8022 dB\n",
      "  Train SE: 0.091993, Val SE: 0.088527\n",
      "  LR: 1.00e-03\n",
      "Epoch 10/55:\n",
      "  Train Loss: 0.837387, Val Loss: 0.814044\n",
      "  Train BER: 0.250809, Val BER: 0.250128\n",
      "  Train SNR: 0.7459 dB, Val SNR: 0.8230 dB\n",
      "  Train SE: 0.092328, Val SE: 0.089122\n",
      "  LR: 1.00e-03\n",
      "Epoch 11/55:\n",
      "  Train Loss: 0.834264, Val Loss: 0.812666\n",
      "  Train BER: 0.250391, Val BER: 0.249909\n",
      "  Train SNR: 0.7632 dB, Val SNR: 0.8280 dB\n",
      "  Train SE: 0.092625, Val SE: 0.088908\n",
      "  LR: 1.00e-03\n",
      "Epoch 12/55:\n",
      "  Train Loss: 0.833322, Val Loss: 0.810095\n",
      "  Train BER: 0.249947, Val BER: 0.248432\n",
      "  Train SNR: 0.7675 dB, Val SNR: 0.8419 dB\n",
      "  Train SE: 0.092380, Val SE: 0.088761\n",
      "  LR: 1.00e-03\n",
      "Epoch 13/55:\n",
      "  Train Loss: 0.830775, Val Loss: 0.809138\n",
      "  Train BER: 0.249744, Val BER: 0.248421\n",
      "  Train SNR: 0.7822 dB, Val SNR: 0.8475 dB\n",
      "  Train SE: 0.092846, Val SE: 0.089445\n",
      "  LR: 1.00e-03\n",
      "Epoch 14/55:\n",
      "  Train Loss: 0.829024, Val Loss: 0.807309\n",
      "  Train BER: 0.248894, Val BER: 0.248505\n",
      "  Train SNR: 0.7903 dB, Val SNR: 0.8558 dB\n",
      "  Train SE: 0.092661, Val SE: 0.088659\n",
      "  LR: 1.00e-03\n",
      "Epoch 15/55:\n",
      "  Train Loss: 0.828238, Val Loss: 0.806336\n",
      "  Train BER: 0.248134, Val BER: 0.248262\n",
      "  Train SNR: 0.7949 dB, Val SNR: 0.8617 dB\n",
      "  Train SE: 0.092930, Val SE: 0.089653\n",
      "  LR: 1.00e-03\n",
      "Epoch 16/55:\n",
      "  Train Loss: 0.825930, Val Loss: 0.804526\n",
      "  Train BER: 0.247867, Val BER: 0.247303\n",
      "  Train SNR: 0.8069 dB, Val SNR: 0.8719 dB\n",
      "  Train SE: 0.093206, Val SE: 0.090683\n",
      "  LR: 1.00e-03\n",
      "Epoch 17/55:\n",
      "  Train Loss: 0.823986, Val Loss: 0.804043\n",
      "  Train BER: 0.248031, Val BER: 0.247694\n",
      "  Train SNR: 0.8170 dB, Val SNR: 0.8741 dB\n",
      "  Train SE: 0.093546, Val SE: 0.090263\n",
      "  LR: 1.00e-03\n",
      "Epoch 18/55:\n",
      "  Train Loss: 0.822486, Val Loss: 0.803172\n",
      "  Train BER: 0.247753, Val BER: 0.247774\n",
      "  Train SNR: 0.8254 dB, Val SNR: 0.8783 dB\n",
      "  Train SE: 0.094090, Val SE: 0.090188\n",
      "  LR: 1.00e-03\n",
      "Epoch 19/55:\n",
      "  Train Loss: 0.821782, Val Loss: 0.801473\n",
      "  Train BER: 0.247586, Val BER: 0.246991\n",
      "  Train SNR: 0.8278 dB, Val SNR: 0.8874 dB\n",
      "  Train SE: 0.094328, Val SE: 0.090638\n",
      "  LR: 1.00e-03\n",
      "Epoch 20/55:\n",
      "  Train Loss: 0.819883, Val Loss: 0.800984\n",
      "  Train BER: 0.246524, Val BER: 0.247571\n",
      "  Train SNR: 0.8387 dB, Val SNR: 0.8891 dB\n",
      "  Train SE: 0.093937, Val SE: 0.090087\n",
      "  LR: 1.00e-03\n",
      "Epoch 21/55:\n",
      "  Train Loss: 0.820567, Val Loss: 0.800895\n",
      "  Train BER: 0.246955, Val BER: 0.247230\n",
      "  Train SNR: 0.8349 dB, Val SNR: 0.8901 dB\n",
      "  Train SE: 0.093933, Val SE: 0.089687\n",
      "  LR: 1.00e-03\n",
      "Epoch 22/55:\n",
      "  Train Loss: 0.817693, Val Loss: 0.798746\n",
      "  Train BER: 0.246410, Val BER: 0.246748\n",
      "  Train SNR: 0.8496 dB, Val SNR: 0.9016 dB\n",
      "  Train SE: 0.094451, Val SE: 0.091122\n",
      "  LR: 1.00e-03\n",
      "Epoch 23/55:\n",
      "  Train Loss: 0.816739, Val Loss: 0.800263\n",
      "  Train BER: 0.246159, Val BER: 0.247473\n",
      "  Train SNR: 0.8551 dB, Val SNR: 0.8929 dB\n",
      "  Train SE: 0.094884, Val SE: 0.089850\n",
      "  LR: 1.00e-03\n",
      "Epoch 24/55:\n",
      "  Train Loss: 0.816881, Val Loss: 0.798170\n",
      "  Train BER: 0.246259, Val BER: 0.247230\n",
      "  Train SNR: 0.8537 dB, Val SNR: 0.9051 dB\n",
      "  Train SE: 0.094431, Val SE: 0.090604\n",
      "  LR: 1.00e-03\n",
      "Epoch 25/55:\n",
      "  Train Loss: 0.815206, Val Loss: 0.798384\n",
      "  Train BER: 0.246335, Val BER: 0.247128\n",
      "  Train SNR: 0.8632 dB, Val SNR: 0.9027 dB\n",
      "  Train SE: 0.094305, Val SE: 0.091196\n",
      "  LR: 1.00e-03\n",
      "Epoch 26/55:\n",
      "  Train Loss: 0.814526, Val Loss: 0.797945\n",
      "  Train BER: 0.246227, Val BER: 0.247172\n",
      "  Train SNR: 0.8663 dB, Val SNR: 0.9063 dB\n",
      "  Train SE: 0.094540, Val SE: 0.090900\n",
      "  LR: 1.00e-03\n",
      "Epoch 27/55:\n",
      "  Train Loss: 0.814917, Val Loss: 0.797044\n",
      "  Train BER: 0.245666, Val BER: 0.246605\n",
      "  Train SNR: 0.8649 dB, Val SNR: 0.9108 dB\n",
      "  Train SE: 0.094773, Val SE: 0.090958\n",
      "  LR: 1.00e-03\n",
      "Epoch 28/55:\n",
      "  Train Loss: 0.813123, Val Loss: 0.797642\n",
      "  Train BER: 0.245379, Val BER: 0.247197\n",
      "  Train SNR: 0.8740 dB, Val SNR: 0.9073 dB\n",
      "  Train SE: 0.094685, Val SE: 0.091765\n",
      "  LR: 1.00e-03\n",
      "Epoch 29/55:\n",
      "  Train Loss: 0.813108, Val Loss: 0.795975\n",
      "  Train BER: 0.245573, Val BER: 0.246592\n",
      "  Train SNR: 0.8744 dB, Val SNR: 0.9160 dB\n",
      "  Train SE: 0.094742, Val SE: 0.091296\n",
      "  LR: 1.00e-03\n",
      "Epoch 30/55:\n",
      "  Train Loss: 0.812310, Val Loss: 0.794367\n",
      "  Train BER: 0.245627, Val BER: 0.246612\n",
      "  Train SNR: 0.8785 dB, Val SNR: 0.9248 dB\n",
      "  Train SE: 0.095097, Val SE: 0.090671\n",
      "  LR: 1.00e-03\n",
      "Epoch 31/55:\n",
      "  Train Loss: 0.812310, Val Loss: 0.794449\n",
      "  Train BER: 0.245106, Val BER: 0.246882\n",
      "  Train SNR: 0.8788 dB, Val SNR: 0.9238 dB\n",
      "  Train SE: 0.094891, Val SE: 0.091601\n",
      "  LR: 1.00e-03\n",
      "Epoch 32/55:\n",
      "  Train Loss: 0.810921, Val Loss: 0.794387\n",
      "  Train BER: 0.244870, Val BER: 0.245790\n",
      "  Train SNR: 0.8861 dB, Val SNR: 0.9253 dB\n",
      "  Train SE: 0.095019, Val SE: 0.092097\n",
      "  LR: 1.00e-03\n",
      "Epoch 33/55:\n",
      "  Train Loss: 0.810508, Val Loss: 0.795337\n",
      "  Train BER: 0.244652, Val BER: 0.247475\n",
      "  Train SNR: 0.8882 dB, Val SNR: 0.9193 dB\n",
      "  Train SE: 0.094897, Val SE: 0.091183\n",
      "  LR: 1.00e-03\n",
      "Epoch 34/55:\n",
      "  Train Loss: 0.809991, Val Loss: 0.794591\n",
      "  Train BER: 0.244930, Val BER: 0.246822\n",
      "  Train SNR: 0.8915 dB, Val SNR: 0.9236 dB\n",
      "  Train SE: 0.095001, Val SE: 0.090999\n",
      "  LR: 5.00e-04\n",
      "Epoch 35/55:\n",
      "  Train Loss: 0.808641, Val Loss: 0.792755\n",
      "  Train BER: 0.244697, Val BER: 0.246056\n",
      "  Train SNR: 0.8985 dB, Val SNR: 0.9345 dB\n",
      "  Train SE: 0.095325, Val SE: 0.092309\n",
      "  LR: 5.00e-04\n",
      "Epoch 36/55:\n",
      "  Train Loss: 0.808669, Val Loss: 0.792698\n",
      "  Train BER: 0.243868, Val BER: 0.246497\n",
      "  Train SNR: 0.8983 dB, Val SNR: 0.9342 dB\n",
      "  Train SE: 0.095508, Val SE: 0.091819\n",
      "  LR: 5.00e-04\n",
      "Epoch 37/55:\n",
      "  Train Loss: 0.807233, Val Loss: 0.792643\n",
      "  Train BER: 0.244175, Val BER: 0.246597\n",
      "  Train SNR: 0.9062 dB, Val SNR: 0.9341 dB\n",
      "  Train SE: 0.095514, Val SE: 0.091964\n",
      "  LR: 5.00e-04\n",
      "Epoch 38/55:\n",
      "  Train Loss: 0.808216, Val Loss: 0.792029\n",
      "  Train BER: 0.244261, Val BER: 0.245972\n",
      "  Train SNR: 0.9008 dB, Val SNR: 0.9378 dB\n",
      "  Train SE: 0.094921, Val SE: 0.091762\n",
      "  LR: 5.00e-04\n",
      "Epoch 39/55:\n",
      "  Train Loss: 0.807366, Val Loss: 0.792162\n",
      "  Train BER: 0.243715, Val BER: 0.246546\n",
      "  Train SNR: 0.9064 dB, Val SNR: 0.9367 dB\n",
      "  Train SE: 0.095413, Val SE: 0.091236\n",
      "  LR: 5.00e-04\n",
      "Epoch 40/55:\n",
      "  Train Loss: 0.807953, Val Loss: 0.791667\n",
      "  Train BER: 0.244230, Val BER: 0.246373\n",
      "  Train SNR: 0.9022 dB, Val SNR: 0.9395 dB\n",
      "  Train SE: 0.095584, Val SE: 0.091759\n",
      "  LR: 5.00e-04\n",
      "Epoch 41/55:\n",
      "  Train Loss: 0.807686, Val Loss: 0.791877\n",
      "  Train BER: 0.244167, Val BER: 0.246297\n",
      "  Train SNR: 0.9033 dB, Val SNR: 0.9386 dB\n",
      "  Train SE: 0.095627, Val SE: 0.091870\n",
      "  LR: 5.00e-04\n",
      "Epoch 42/55:\n",
      "  Train Loss: 0.806213, Val Loss: 0.791326\n",
      "  Train BER: 0.243861, Val BER: 0.246149\n",
      "  Train SNR: 0.9108 dB, Val SNR: 0.9426 dB\n",
      "  Train SE: 0.095933, Val SE: 0.092760\n",
      "  LR: 5.00e-04\n",
      "Epoch 43/55:\n",
      "  Train Loss: 0.806693, Val Loss: 0.792853\n",
      "  Train BER: 0.244136, Val BER: 0.246825\n",
      "  Train SNR: 0.9091 dB, Val SNR: 0.9337 dB\n",
      "  Train SE: 0.095201, Val SE: 0.092392\n",
      "  LR: 5.00e-04\n",
      "Epoch 44/55:\n",
      "  Train Loss: 0.806688, Val Loss: 0.792067\n",
      "  Train BER: 0.244439, Val BER: 0.246096\n",
      "  Train SNR: 0.9093 dB, Val SNR: 0.9385 dB\n",
      "  Train SE: 0.095573, Val SE: 0.092577\n",
      "  LR: 5.00e-04\n",
      "Epoch 45/55:\n",
      "  Train Loss: 0.805750, Val Loss: 0.790913\n",
      "  Train BER: 0.244404, Val BER: 0.246228\n",
      "  Train SNR: 0.9139 dB, Val SNR: 0.9443 dB\n",
      "  Train SE: 0.095598, Val SE: 0.091542\n",
      "  LR: 5.00e-04\n",
      "Epoch 46/55:\n",
      "  Train Loss: 0.805735, Val Loss: 0.791121\n",
      "  Train BER: 0.244021, Val BER: 0.246619\n",
      "  Train SNR: 0.9137 dB, Val SNR: 0.9422 dB\n",
      "  Train SE: 0.095470, Val SE: 0.092579\n",
      "  LR: 5.00e-04\n",
      "Epoch 47/55:\n",
      "  Train Loss: 0.804625, Val Loss: 0.790120\n",
      "  Train BER: 0.243590, Val BER: 0.246310\n",
      "  Train SNR: 0.9204 dB, Val SNR: 0.9480 dB\n",
      "  Train SE: 0.095394, Val SE: 0.092144\n",
      "  LR: 5.00e-04\n",
      "Epoch 48/55:\n",
      "  Train Loss: 0.806804, Val Loss: 0.791735\n",
      "  Train BER: 0.243674, Val BER: 0.246562\n",
      "  Train SNR: 0.9080 dB, Val SNR: 0.9393 dB\n",
      "  Train SE: 0.095480, Val SE: 0.092488\n",
      "  LR: 5.00e-04\n",
      "Epoch 49/55:\n",
      "  Train Loss: 0.805378, Val Loss: 0.790513\n",
      "  Train BER: 0.243609, Val BER: 0.246399\n",
      "  Train SNR: 0.9163 dB, Val SNR: 0.9463 dB\n",
      "  Train SE: 0.095553, Val SE: 0.092337\n",
      "  LR: 5.00e-04\n",
      "Epoch 50/55:\n",
      "  Train Loss: 0.805387, Val Loss: 0.791412\n",
      "  Train BER: 0.243809, Val BER: 0.246630\n",
      "  Train SNR: 0.9167 dB, Val SNR: 0.9406 dB\n",
      "  Train SE: 0.096086, Val SE: 0.092171\n",
      "  LR: 5.00e-04\n",
      "Epoch 51/55:\n",
      "  Train Loss: 0.805843, Val Loss: 0.790862\n",
      "  Train BER: 0.244370, Val BER: 0.246152\n",
      "  Train SNR: 0.9142 dB, Val SNR: 0.9443 dB\n",
      "  Train SE: 0.095461, Val SE: 0.092391\n",
      "  LR: 2.50e-04\n",
      "Epoch 52/55:\n",
      "  Train Loss: 0.803712, Val Loss: 0.790085\n",
      "  Train BER: 0.243120, Val BER: 0.246481\n",
      "  Train SNR: 0.9246 dB, Val SNR: 0.9487 dB\n",
      "  Train SE: 0.095826, Val SE: 0.092178\n",
      "  LR: 2.50e-04\n",
      "Epoch 53/55:\n",
      "  Train Loss: 0.804099, Val Loss: 0.789440\n",
      "  Train BER: 0.243321, Val BER: 0.245672\n",
      "  Train SNR: 0.9236 dB, Val SNR: 0.9518 dB\n",
      "  Train SE: 0.095821, Val SE: 0.092965\n",
      "  LR: 2.50e-04\n",
      "Epoch 54/55:\n",
      "  Train Loss: 0.804071, Val Loss: 0.790029\n",
      "  Train BER: 0.243552, Val BER: 0.246100\n",
      "  Train SNR: 0.9242 dB, Val SNR: 0.9488 dB\n",
      "  Train SE: 0.095972, Val SE: 0.093385\n",
      "  LR: 2.50e-04\n",
      "Epoch 55/55:\n",
      "  Train Loss: 0.803771, Val Loss: 0.789633\n",
      "  Train BER: 0.243536, Val BER: 0.245765\n",
      "  Train SNR: 0.9249 dB, Val SNR: 0.9509 dB\n",
      "  Train SE: 0.095464, Val SE: 0.092262\n",
      "  LR: 2.50e-04\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 42\u001b[39m\n\u001b[32m     34\u001b[39m model = DLinear_NN(\n\u001b[32m     35\u001b[39m     pred_len=\u001b[32m8\u001b[39m,        \n\u001b[32m     36\u001b[39m     seq_len=\u001b[32m160\u001b[39m,       \n\u001b[32m     37\u001b[39m     num_features=num_features,\n\u001b[32m     38\u001b[39m ).to(device)\n\u001b[32m     40\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mКоличество параметров: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(p.numel()\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mp\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mmodel.parameters())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m model, train_losses, val_losses = train_model(\n\u001b[32m     43\u001b[39m     model, train_loader, val_loader, device, EPOCHS, LEARNING_RATE\n\u001b[32m     44\u001b[39m )\n\u001b[32m     46\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m3. Прогнозирование...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     47\u001b[39m test_predictions = predict(model, test_loader, device)\n",
      "\u001b[31mValueError\u001b[39m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": [
    "from model import DLinear_NN\n",
    "\n",
    "SEQUENCE_LENGTH = 160\n",
    "HORIZON = 8\n",
    "BATCH_SIZE = 1280\n",
    "EPOCHS = 55\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "df = load_and_prepare_data()\n",
    "df = create_features(df)\n",
    "df_for_ts, feature_columns = prepare_time_series_data(df)\n",
    "\n",
    "device_ids = df_for_ts['id'].unique()\n",
    "train_devices = device_ids[:int(0.7 * len(device_ids))]\n",
    "val_devices = device_ids[int(0.7 * len(device_ids)):int(0.85 * len(device_ids))]\n",
    "test_devices = device_ids[int(0.85 * len(device_ids)):]\n",
    "\n",
    "train_data = df_for_ts[df_for_ts['id'].isin(train_devices)]\n",
    "val_data = df_for_ts[df_for_ts['id'].isin(val_devices)]\n",
    "test_data = df_for_ts[df_for_ts['id'].isin(test_devices)]\n",
    "\n",
    "train_dataset = TimeSeriesDataset(train_data, SEQUENCE_LENGTH, HORIZON)\n",
    "val_dataset = TimeSeriesDataset(val_data, SEQUENCE_LENGTH, HORIZON)\n",
    "test_dataset = TimeSeriesDataset(test_data, SEQUENCE_LENGTH, HORIZON)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Модель\n",
    "num_features = len(feature_columns) - 1\n",
    "model = DLinear_NN(\n",
    "    pred_len=8,        \n",
    "    seq_len=160,       \n",
    "    num_features=num_features,\n",
    ").to(device)\n",
    "\n",
    "print(f\"Количество параметров: {sum(p.numel() for p in model.parameters())}\")\n",
    "\n",
    "model, train_losses, val_losses = train_model(\n",
    "    model, train_loader, val_loader, device, EPOCHS, LEARNING_RATE\n",
    ")\n",
    "\n",
    "print(\"3. Прогнозирование...\")\n",
    "test_predictions = predict(model, test_loader, device)\n",
    "\n",
    "test_predictions_original = []\n",
    "for i, device_id in enumerate(test_devices):\n",
    "    if device_id in test_dataset.scalers:\n",
    "        _, target_scaler = test_dataset.scalers[device_id]\n",
    "        if target_scaler:\n",
    "            device_preds = test_predictions[i * (400 - SEQUENCE_LENGTH - HORIZON + 1): \n",
    "                                            (i + 1) * (400 - SEQUENCE_LENGTH - HORIZON + 1)]\n",
    "            last_preds = device_preds[:, -1].reshape(-1, 1)\n",
    "            original_scale = target_scaler.inverse_transform(last_preds).flatten()\n",
    "            test_predictions_original.extend(original_scale)\n",
    "\n",
    "print(\"4. Создание submission...\")\n",
    "create_submission(test_predictions_original)\n",
    "\n",
    "print(\"Готово!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
